\documentclass[11pt]{extarticle}

% meta
\title{Текстовые эмбединги. Информационный поиск. RAG}
\author{\href{https://github.com/voorhs/rag-lecture}{Contribute on GitHub}}
\date{Октябрь 2025.}

% \hrule- XeLaTeX configuration for Cyrillic \hrule-
\usepackage{fontspec}
\setmainfont{DejaVu Serif}
\setsansfont{DejaVu Sans}
\setmonofont{DejaVu Sans Mono}
\usepackage{polyglossia}
\setmainlanguage{russian}
\setotherlanguage{english}
\usepackage{indentfirst}
\usepackage{csquotes}
\usepackage[style=numeric, sorting=none]{biblatex}
\addbibresource{references.bib}

% page settings
\usepackage[
    left=1.8cm,
    right=1.8cm,
    top=1.8cm,
    bottom=1.8cm,
    bindingoffset=0cm
]{geometry}

\usepackage{graphicx, hyperref, xcolor}
\hypersetup{
    colorlinks=true,
    linkcolor=teal,
    filecolor=magenta, 
    urlcolor=blue,
    citecolor=olive,
    pdftitle={RAG},
    % pdfpagemode=FullScreen,
    linktoc=all
    }

\usepackage{wrapfig,caption}

% figures
\usepackage{caption}
\usepackage{subcaption}
\usepackage{floatrow}
\floatsetup{heightadjust=object}

% math
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools,esint,eucal}

% tables
\usepackage{array}

\begin{document}

\maketitle
\tableofcontents
\newpage

\begin{abstract}
Данная лекция представляет всесторонний обзор State-Space Models (SSMs), прослеживая их эволюцию от основополагающего фреймворка HiPPO до современной архитектуры Mamba. Мы начинаем с математических основ: полиномиальных проекций и дифференциальных уравнений; затем исследуем, как эти концепции были адаптированы в эффективные архитектуры для моделирования последовательностей. Лекция охватывает три основных развития: рекуррентную память HiPPO с оптимальными полиномиальными проекциями, переход к структурированным слоям пространства состояний (LSSL, S4, DSS, S4D), и наконец, селективные модели пространства состояний, воплощенные в Mamba. На протяжении всего изложения мы рассматриваем как теоретические основы, так и практические реализации, подчеркивая, как эти модели достигают линейной временной сложности, сохраняя при этом сопоставимую производительность с трансформерами на различных задачах моделирования последовательностей.
\end{abstract}

\section{Математические предпосылки}

Сперва мы вспомним математические основы, которые пригодятся для понимания State-Space Models.

\subsection{Математические основы}

\subsubsection{Разложение по базису в векторных пространствах}

Рассмотрим векторное пространство $V(\mathbb{R})$ с базисом $B = \{\vec{b}_1, \ldots, \vec{b}_N\}$. Любой вектор $\vec{a} \in V$ может быть выражен как:

$$\vec{a} = c_1\vec{b}_1 + \ldots + c_N\vec{b}_N$$

где $\vec{c} = (c_1, \ldots, c_N)$ — это вектор координат, представляющий $\vec{a}$ в базисе $B$.

Если базис ортонормированный:
$$\langle \vec{b}_i, \vec{b}_j \rangle = \begin{cases} 1, & i = j \\ 0, & i \neq j \end{cases}$$

то коэффициенты могут быть вычислены как:
$$c_n = \langle \vec{a}, \vec{b}_n \rangle$$

При ортонормированном базисе векторы координат $\vec c$ образуют векторное пространство $\mathbb{R}^N$. Если допустить некоторую нестрогость интерпретации, то с точки зрения машинного обучения это означает, что вектор координат является идеальным признаковым описанием исходного вектора.

\subsubsection{Функциональные пространства и ортонормированные базисы}

Те же принципы распространяются на функциональные пространства. Для функций $f, g \in L^2[-1,1]$ мы определяем скалярное произведение как

$$\langle f, g \rangle = \int_{-1}^1 f(x)g(x) w(x) dx,$$

где $w(x)$ — весовая функция. Классическими примерами ортонормированных полиномиальных базисов служат полиномы Лежандра, полиномы Лагера, полиномы Чебышева и полиномы Эрмита.

\begin{table}[h]
\centering
\small
\renewcommand{\arraystretch}{2.5}
\begin{tabular}{m{0.26\linewidth}|>{\centering\arraybackslash}m{0.15\linewidth}|>{\centering\arraybackslash}m{0.18\linewidth}|>{\centering\arraybackslash}m{0.35\linewidth}}
\textbf{Название} & \textbf{Простр.} & \textbf{Весовая ф-я} & \textbf{Явная формула} \\
\hline
Тригонометрический ряд & $L_2[0, 2\pi]$ & $w(x)=1$ & $\displaystyle\{\exp(inx)\}_{i=-\infty}^{+\infty}$ \\
\hline
Полиномы Лежандра & $L^2[-1,1]$ & $w(x)=1$ & $\displaystyle P_n(x)=\frac{1}{2^nn!}\frac{d^n}{dx^n}(x^2-1)^n$ \\
\hline
Полиномы Лагера & $L^2[0,+\infty)$ & $w(x)=e^{-x}$ & $\displaystyle L_n(x)=\frac{e^x}{n!}\frac{d^n}{dx^n}(e^n x^n)$ \\
\hline
Полиномы Чебышева & $L^2[-1,1]$ & $\displaystyle w(x)=\frac{1}{\sqrt{1-x^2}}$ & $\displaystyle T_n(x)=\sum_{j=0}^{\lfloor n/2\rfloor}\binom{n}{2j}(x^2-1)^jx^{n-2j}$ \\
\hline
Полиномы Эрмита & $L^2(-\infty,+\infty)$ & $w(x)=e^{-x^2}$ & $\displaystyle H_n(x)=(-1)^ne^{x^2}\frac{d^n}{dx^n}e^{-x^2}$ \\
\end{tabular}
\caption{Ортонормированные полиномиальные базисы}
\end{table}

\subsubsection{Полиномиальные проекции}

Отличие пространства Лебега от векторного пространства в предыдущем разделе в том, что базис $L^2$ бесконечный, в то время как базис $V(\mathbb{R})$ конечный. Это означает, что идеальное признаковое описание функции будет бесконечномерным вектором. На первый взгляд, это ставит непреодолимое препятствие для практических применений. Однако мы можем воспользоваться аппроксимациями.

Для любой функции $f(x) \in L^2[-1,1]$ и ортонормированного базиса $\{P_n(x)\}_{n=1}^\infty$ справедлива формула оптимальной аппроксимации:

$$f(x) = \sum_{n=1}^\infty c_n P_n(x) \approx \sum_{n=1}^N c_n P_n(x)$$

где коэффициенты:
$$c_n = \langle f(x), P_n(x) \rangle = \int_{-1}^1 f(x)P_n(x)w(x)dx$$

В терминах алгебры такая аппроксимация является проекцией пространства $L^2$ на подпространство, образованное конечным базисом $\{P_n\}_{n=1}^N$. Чем больше $N$, тем точнее аппроксимация.

\subsubsection{Обыкновенные дифференциальные уравнения (ОДУ)}

ОДУ описывают динамику системы:
$$\frac{d}{dt}x(t) = f(x(t), t)$$

Решение ОДУ (численно или аналитически) называется интегрированием.

Простейшую схему численного интегрирования дает метод Эйлера:
$$
  \frac{x(t+dt) - x(t)}{dt} = f(x(t), t),\\
  x_{k+1} = x_k + f(x_k, k)dt
$$

где $dt$ \hrule т.н. шаг дискретизации. Для интегрирования методом Эйлера достаточно задать начальное условие $x_0$ и запустить рекуррентный пересчет по формуле для $x_{k+1}$. Физически этот процесс можно интерпретировать как покадровую отрисовку движения некоторой динамической системы.

\subsubsection{Временные ряды}

Одномерным временным рядом (univariate time series) мы будем называть числовую последовательность $x_1,\ldots,x_T$. Число $x_t\in\mathbb{R}$ мы называем наблюдением, или замером, (observation) в момент времени $t$ (timestamp). Эти названия отражают, в каких областях обычно возникают временные ряды: замер температуры воздуха, концентрации вещества и проч.

Многомерным временным рядом (multivariate time series) мы будем называть последовательность векторов $x_1,\ldots,x_T$. В данном случае наблюдением будет целый вектор $x_t\in\mathbb{R}^C$. Он представляет собой замеры сразу нескольких величин. Например, вместе с температурой можно мерить давление и влажность воздуха. Отличие $C$-мерного временного ряда от набора из $C$ одномерных временных рядов в том, что в многомерном ряде обычно предполагают, что компоненты вектора $x_t$ относятся к одному моменту времени.

\subsubsection{Рекуррентные нейронные сети}

Простейшая рекуррентная сеть задается следующими формулами:
\begin{align}
\text{RNN}(h,x)&=(1-g)\circ h + g\circ\tanh(W_1h+U_1x+b_1),\\
g&=\sigma(W_2h+U_2x+b_2)
\end{align}

% \begin{figure}[h]
% \centering
% \includesvg[width=0.8\textwidth]{figures/rnn-cell.drawio}
% \end{figure}


\section{HiPPO: Рекуррентная память с оптимальными полиномиальными проекциями}

\subsection{Фреймворк}

Сделаем предположение: пусть за всяким одномерным временным рядом $\{x_i\}_{i=1}^T$ стоит некоторая непрерывная функция $f(t)$, определенная при $t\in[0,+\infty)$. Временной ряд при этом будем называть дискретизацией функции $f(t)$ по некоторой сетке значений $t$. Основная идея HiPPO в том, чтобы для временного ряда $\{x_i\}_{i=1}^T$ получить коэффициенты $\vec c\in\mathbb{R}^N$ полиномиальной проекции функции $f(t)$ \hrule тогда эти коэффициенты можно использовать в ML приложениях как признаковое описание временного ряда. Проблема лишь в том, что у нас нет доступа к $f(t)$, мы видим лишь дискретизацию $\{x_i\}_{i=1}^N$.

% \begin{figure}[h]
% \centering
% \includesvg[width=0.8\textwidth]{figures/continuous-function-1-02}
% \end{figure}

Пусть $\vec c(\tau)\in\mathbb{R}^N$ \hrule вектор коэффициентов полиномиальной проекции функции $f(t)\big|_{[0,\tau]}$. Оказывается, что векторная функция $\vec c(\tau)$ описывается ОДУ:

$$\frac{d}{dt}\vec c(t) = Ac(t) + Bf(t)$$

где $A\in\mathbb{R}^{N\times N}$ и $B\in\mathbb{R}^N$ \hrule некоторые константы, определяемые выбранным полиномиальным базисом.

Применяя численное интегрирование методом Эйлера:
$$c_{k+1} = c_k + (Ac_k + Bf_k)dt = (I + Adt)c_k + (Bdt)f_k$$

Обозначая $\bar{A} = I + Adt$ и $\bar{B} = Bdt$, получаем:
$$c_{k+1} = \bar{A}c_{k-1} + \bar{B}f_k$$

В итоговом выражении участвует величина $f_k=x_k$ \hrule один отсчет временного ряда. Таким образом, для вычисления признакового описания временного ряда, достаточно произвести пересчет по рекуррентным формулам для $c_{k+1}$. В этом и заключается HiPPO \hrule High Order Polynomial Projections. Этот метод стремится решать задачи на временных рядах с помощью рекуррентной памяти, используя коэффициенты разложения как скрытые состояния. Они служат математическим обоснованием оптимальности сжатия исторической информации.

\subsection{Реализации HiPPO}

В зависимости от выбранного базиса, мы получаем разные $\bar A$ и $\bar B$. Конкретные значения выводятся аналитически. Ниже мы приводим значения, которые получили авторы HiPPO.

\textbf{Translated Legendre (LegT)}
\begin{itemize}
\item Пространство: $L^2[\tau-\theta, \tau]$ (скользящее окно)
\item Весовая функция: $w(t) = \frac{1}{\theta}[\tau-\theta \leq t \leq \tau]$
\item Элементы матрицы:
  $$A_{nk} = \frac{1}{\theta}\begin{cases}
  (-1)^{n-k}(2n+1), & n \geq k \\
  2n+1, & n \leq k
  \end{cases}$$
\end{itemize}

\textbf{Translated Laguerre (LagT)}
\begin{itemize}
\item Пространство: $L^2[-\infty, \tau]$ (экспоненциальное взвешивание)
\item Весовая функция: $w(t) = \exp(t-\tau)[t \leq \tau]$
\item Элементы матрицы:
  $$A_{nk} = \begin{cases}
  1, & n \geq k \\
  0, & n < k
  \end{cases}$$
\end{itemize}

\textbf{Scaled Legendre (LegS)}
\begin{itemize}
\item Пространство: $L^2[0, \tau]$ (полная история)
\item Весовая функция: $w(t) = \frac{1}{\tau}[0 \leq t \leq \tau]$
\item Элементы матрицы:
  $$A_{nk} = -\frac{1}{\tau}\begin{cases}
  \sqrt{(2n+1)(2k+1)}, & n > k \\
  n+1, & n = k \\
  0, & n < k
  \end{cases}$$
\end{itemize}

\subsection{Интеграция с нейронными сетями и эксперименты}

Для экспериментальной проверки HiPPO был интегрирован с RNN путем использования коэффициентов разложения как дополнительного контекста:

$$\text{RNN}(h, [c, x])$$

где $c_t = \bar{A}c_{t-1} + \bar{B}f_t$ представляет коэффициенты HiPPO для $f(t) = w^T h_t$ (с обучаемым $w$).

% \begin{figure}[h]
% \centering
% \includesvg[width=0.8\textwidth]{figures/rnn-cell-hippo}
% \end{figure}

В свое время HiPPO достиг SOTA на ряде задач: на Permuted MNIST измерялась точность классификации изображений как классификация временного ряда; в задаче прогнозировании для хаотической системы Mackey–Glass; на IMDB Sentiment классификация текстов. Ключевые преимущества HiPPO \hrule быстрый рекуррентный инференс и теоретические гарантии точности аппроксимации. На задачах временных рядов HiPPO показал результаты, сопоставимые не только с RNN, но и с трансформером, превосходя их по скорости инференса в десятки и сотни раз.

\section{От теории к практике}

\subsection{Linear State-Space Layer (LSSL)}

Эксперименты HiPPO проводились на практически синтетических и игрушечных датасетах. Основываясь на HiPPO, LSSL трансформирует теоретический фреймворк HiPPO в слой нейронной сети, пригодный для более серьезных применений.

Авторы определяют отображение входа $\{u_t\,|\,u_t\in\mathbb{R}\}$ в выход $\{y_t\,|\,y_t\in\mathbb{R}^M\}$:
\begin{align}
  x_t &= \bar{A} x_{t-1} + \bar{B} u_t \\
  y_t &= Cx_t + Du_t
\end{align}

где:
\begin{itemize}
\item $\bar{A} \in \mathbb{R}^{N \times N}$, $\bar{B} \in \mathbb{R}^N$ инициализируется из HiPPO матриц (опционально обучаемые, но об этом позже)
\item $C \in \mathbb{R}^{M \times N}$, $D \in \mathbb{R}^M$ (обучаемые)
\end{itemize}

В теории оптимального управления подобные системы называют моделями пространства состояний \hrule state-space model (SSM).

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/lssl-sequence-mapping.png}
\end{figure}

Используя SSM, авторы HiPPO построили Linear State-Space Layer (LSSL):

% \begin{figure}[h]
% \centering
% \includesvg[width=0.8\textwidth]{figures/continuous-function-1-lssl}
% \end{figure}

Авторы стремились реализовать компоненты, напоминающие компоненты трансформерного слоя:
\begin{itemize}
\item коммуникация между токенами
\item position-wise обработка
\item add \& norm
\end{itemize}

\textbf{Рекуррентность как свертка}

Один из минусов рекуррентной архитектуры в сравнении с трансформерами \hrule это невозможность параллелизовать обучение. Наличие цикла по $t$ обеспечивает быстрый инференс, например, для задачи next token prediction, но ужасно неэффективно, когда последовательность уже известна на перед, как на обучении. Однако у LSSL есть теоретический способ для ускорения.

Если развернуть рекуррентную формулу, то мы увидим что SSM реализуют свертку:
$$y_t = C(\bar{A})^t\bar{B}u_0 + C(\bar{A})^{t-1}\bar{B}u_1 + \ldots + C\bar{B}u_t+Du_t$$

Это может быть записано как:
$$y = \mathcal{K}_L(\bar{A}, \bar{B}, C) * u + Du$$

где ядро:
$$\mathcal{K}_L(A, B, C) = (CB, CAB, \ldots, CA^{L-1}B) \in \mathbb{R}^{M \times L}$$

Это ядро свертки отличается от того, что мы привыкли видеть в DL. Обычно мы имеем дело с ядрами фиксированного размера, такими как $3\times 3$ (в компьютерном зрении) или одномерные ядра (в текстовых CNN-классификаторах). Тут же ядро $\mathcal{K}$ имеет длину, равную длине входной последовательности.

Привычные ядра фиксированного размера хорошо параллелятся за счет сведения к матричным операциям. Ядра наподобие $\mathcal{K}$ параллелятся за счет использования FFT (Fast Fourier Transform, алгоритм "бабочка"). Тогда обучение выполняется за $O(L \log L)$. А на инференсе у нас есть выбор: либо последовательно за $O(L)$ для задач в духе next token prediction, либо $O(L\log L)$ для задач в духе классификации.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/lssl-benefits.png}
\end{figure}

\subsection{Стратегии параметризации}

Выше упоминалось, что матрицу $A$ можно обучать. Согласно экспериментам оригинальной статьи, это привносит ожидаемую специализацию под датасеты и дает небольшой прирост качества. Однако если делать матрицу $A$ обучаемой, то мы обязаны пересчитывать ядро $\mathcal{K}$ после каждого шага оптимизации, в то время как без обучения $A$ достаточно предпосчитать $\mathcal{K}$ один раз.

Чтобы решить проблему накладных расходов, связанных с высчитыванием $\mathcal{K}$, авторы применили трюк с репараметризацией. Идея в том, чтобы ограничить $A$ некоторым классом матриц, в котором находятся оригинальные матрицы HiPPO.

\textbf{Трехдиагональная параметризация (LSSL)}

Матрицы HiPPO могут быть представлены как:
$$A = P(D + T^{-1})Q$$

где $D$, $P$, $Q$ — диагональные, а $T$ — трехдиагональная. Это сокращает параметры с $N^2$ до $6N$, сохраняя теоретические свойства. Более того, подсчет ядра с таким $A$ становится быстрее, если правильно определить порядок матрично-векторных операций.

\textbf{Normal Plus Low-Rank (S4)}

Одна из последующих работ (Structured State-Space Sequence Models, S4) использует параметризацию:
$$A = V\Lambda V^* - PQ^*$$

где $\Lambda$ — диагональная, $V$ — унитарная, а $P, Q \in \mathbb{R}^{N \times r}$ — матрицы низкого ранга. Это позволяет эффективное вычисление ядра через специализированные алгоритмы.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/s4-kernel.png}
\end{figure}

\textbf{Диагональная параметризация (DSS, S4D)}

Самый простой подход использует диагональные матрицы:
$$A = \text{diag}(\lambda_1, \ldots, \lambda_N)$$

Это дает чрезвычайно эффективное вычисление ядра:
If $A=V\Lambda V^{-1}$, then $\exists w\in\mathbb{C}^N$:
$$
\mathcal{K}_L(\bar A, \bar B, C)\Leftrightarrow \mathcal{K}_L(\Lambda, (e^{L\lambda_i dt}-1)_{i=1}^N, w)=w\Lambda^{-1}\text{softmax}(P),
$$
where $P\in\mathbb{C}^{N\times L}$ such that $P_{ij}=\lambda_ij\cdot dt$.


\subsection{Экспериментальные результаты}

На Long‑Range Arena (классификация очень длинных последовательностей) S4 показывает прорывное качество, заметно опережая прежние подходы. В сравнении с трансформерами, в языковом моделировании демонстрируется сопоставимая перплексия при генерации, которая выполняется на порядки быстрее (до 60×). Кроме того, модель успешно обучается напрямую на сырых аудиосигналах с дискретизацией 16 кГц, без спектральной предобработки.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/s4-clm.png}
\end{figure}

\hrule

\section{Mamba: Селективные пространства состояний}

\subsection{Мотивация и постановка проблемы}

Хотя SSM показали перспективность, они страдали от фундаментального ограничения: \textbf{инвариантности по времени}. Параметры $A$, $B$, $C$ оставались постоянными независимо от входа, что затрудняло селективное запоминание или забывание информации на основе контекста.

\textbf{Задача селективного копирования}

Рассмотрим задачу, где модель должна селективно копировать токены на основе контекста. Традиционные SSM испытывают трудности, поскольку не могут адаптировать свой механизм памяти к содержимому входа. 

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/selective-copying.png}
\end{figure}

Подобные синтетические задачи созданы для того, чтобы изолированно проверять способности моделей, которые исследователям кажутся ключевыми. Аналогичная ситуация с другими двумя синтетическими задачами: inductive heads и associative recall.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/synthetic-lm-tasks.png}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/synthetic-lm-eval.png}
\end{figure}

\subsection{Предыстория: Linear Attention}

До Mamba, Linear Attention показал, как эффективно аппроксимировать внимание Transformer:

Transformer attention $\text{softmax}(QK^T)V$ can be viewed as the following:
$$
O_i=\sum_{j=1}^i\underbrace{\frac{\text{sim}(Q_i,K_j)}{\sum_{t=1}^i\text{sim}(Q_i,K_t)}}_{\alpha_{ij}}V_j=\frac{\sum_{j=1}^i\text{sim}(Q_i,K_j)V_j}{\sum_{t=1}^i\text{sim}(Q_i,K_t)}
$$
where $\text{sim}(q,k)=\text{exp}(q^Tk)$. If you choose $\text{sim}(q,k)=\phi(q)^T\phi(k)$ with some non-linear $\phi:\mathbb{R}^d\to\mathbb{R}^d$, then
$$
O_i=\frac{\sum_{j=1}^i\phi(Q_i)^T\phi(K_j)V_j}{\sum_{t=1}^i\phi(Q_i)^T\phi(K_t)}=\left[\frac{\phi(Q_i)^T\sum_{j=1}^i\phi(K_j)V_j^T}{\phi(Q_i)^T\sum_{j=1}^i\phi(K_j)}\right]^T
$$

Let us define $S_i,Z_i$:
$$
O_i^T=\frac{\phi(Q_i)^T\overbrace{\sum_{j=1}^i\phi(K_j)V_j^T}^{S_i}}{\phi(Q_i)^T\underbrace{\sum_{j=1}^i\phi(K_j)}_{Z_i}}=\frac{\phi(Q_i)^TS_i}{\phi(Q_i)^TZ_i}.
$$

Note, that
\begin{align}
S_i&=S_{i-1}+\phi(K_i)V_i^T,\\
Z_i&=Z_{i-1}+\phi(K_i).
\end{align}
This gives us linear-time inference.

\subsection{Hungry Hungry Hippos (H3)}

H3 связал SSM и языковое моделирование, аппроксимируя внимание компонентами SSM:

\textbf{Слой H3}
$$Q \circ \text{SSM}_{\text{diag}}(\text{SSM}_{\text{shift}}(K) \circ V)$$

Эта архитектура аппроксимирует внимание Transformer средствами SSM, достигает сопоставимых результатов на SuperGLUE и опирается на Flash Convolution для эффективной утилизации GPU.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/h3-layer.png}
\end{figure}

\subsection{Архитектура Mamba}

\textbf{Селективные пространства состояний}

Mamba вводит входозависимые параметры:

\begin{align}
x_t &= \bar{A}(x_t) x_{t-1} + \bar{B}(x_t) u_t \\
y_t &= C(x_t) x_t + D(x_t) u_t
\end{align}

где $A$, $B$, $C$ теперь являются функциями от входа $x_t$.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/mamba-algo.png}
\end{figure}

\textbf{Детали реализации}

Параметры $A$, $B$, $C$ задаются простыми линейными проекциями от входа; вводится селективный механизм (ворота), управляющий потоком информации; реализация опирается на кастомные CUDA‑ядра, учитывающие иерархию памяти.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/mamba-memory-io.png}
\end{figure}

\textbf{Алгоритм параллельного сканирования}

Поскольку свёртка в явном виде недоступна, для распараллеливания рекуррентности используется параллельное сканирование: оно обеспечивает $O(N)$‑параллелизм на обучении, сохраняет линейно‑временной инференс и адаптирует алгоритмы кумулятивной суммы к динамике SSM.

\subsection{Компоненты архитектуры Mamba}

\textbf{Полная архитектура}
Блок Mamba включает линейную проекцию входа к скрытой размерности, селективный SSM (входозависимое обновление состояний), Gated MLP с современными активациями (SiLU/Swish) и заключительную проекцию обратно к исходной размерности.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/mamba-architecture.png}
\end{figure}

\textbf{Оптимизация иерархии памяти}

Для повышения утилизации оборудования Mamba различает быструю on‑chip SRAM для активных вычислений и HBM для хранения параметров, а также использует объединённые (fused) операции, минимизирующие пересылки данных.

\subsection{Экспериментальная оценка}

\textbf{Задачи длинного контекста}

На QA с длинными контекстами и в синтетических тестах (Phonebook, RULER) модель дает сопоставимое качество и лучше экстраполирует длину, чем Transformers.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/pure-mamba-synthetic.png}
\end{figure}

\textbf{Гибридные архитектуры}

Комбинирование с компонентами Transformer даёт «лучшее из двух миров»: эффективность Mamba сочетается с возможностями внимания, что обеспечивает более высокое качество и гибкий дизайн.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/hybrid-mamba-eval-synthetic.png}
\end{figure}

\subsection{Характеристики производительности}

Mamba обеспечивает линейно‑временной инференс и постоянное использование памяти, хорошо экстраполирует длину и демонстрирует сопоставимую перплексию. Ограничения касаются in‑context learning (уступает Transformers), меньшей точности «памяти» по сравнению с вниманием и повышенной чувствительности к формату входа.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/denied-mamba.png}
\end{figure}

Оригинальную статью Mamba реджектнули с ICML. Сможете ли вы понять по этой таблице, почему?

\end{document}