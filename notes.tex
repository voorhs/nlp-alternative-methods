\documentclass[11pt]{extarticle}

% meta
\title{Текстовые эмбединги. Информационный поиск. RAG}
\author{\href{https://github.com/voorhs/rag-lecture}{Contribute on GitHub}}
\date{Октябрь 2025.}

% ---- XeLaTeX configuration for Cyrillic \hrule-
\usepackage{fontspec}
\setmainfont{DejaVu Serif}
\setsansfont{DejaVu Sans}
\setmonofont{DejaVu Sans Mono}
\usepackage{polyglossia}
\setmainlanguage{russian}
\setotherlanguage{english}
\usepackage{indentfirst}
\usepackage{csquotes}

\usepackage[style=numeric, sorting=none]{biblatex}
\addbibresource{references.bib}

% page settings
\usepackage[
    left=1.8cm,
    right=1.8cm,
    top=1.8cm,
    bottom=1.8cm,
    bindingoffset=0cm
]{geometry}

\usepackage{graphicx, hyperref, xcolor}
\hypersetup{
    colorlinks=true,
    linkcolor=teal,
    filecolor=magenta, 
    urlcolor=blue,
    citecolor=olive,
    pdftitle={RAG},
    % pdfpagemode=FullScreen,
    linktoc=all
    }

\usepackage{wrapfig,caption}

% figures
\usepackage{caption}
\usepackage{subcaption}
\usepackage{floatrow}
\floatsetup{heightadjust=object}

% math
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools,esint,eucal}

% tables
\usepackage{array}

\begin{document}

\maketitle
\tableofcontents
\newpage

\begin{abstract}
Данная лекция представляет всесторонний обзор State-Space Models (SSMs), прослеживая их эволюцию от основополагающего фреймворка HiPPO~\cite{gu2020hipporecurrentmemoryoptimal} до современной архитектуры Mamba~\cite{gu2024mambalineartimesequencemodeling}. Мы начинаем с математических основ: полиномиальных проекций и дифференциальных уравнений; затем исследуем, как эти концепции были адаптированы в эффективные архитектуры для моделирования последовательностей. Лекция охватывает три основных развития: рекуррентную память HiPPO с оптимальными полиномиальными проекциями, переход к структурированным слоям (LSSL~\cite{gu2021combiningrecurrentconvolutionalcontinuoustime}, S4~\cite{gu2022efficientlymodelinglongsequences}, DSS \cite{gupta2022diagonalstatespaceseffective}, S4D~\cite{gu2022parameterizationinitializationdiagonalstate}), и наконец, селективные модели пространства состояний, воплощенные в Mamba. На протяжении всего изложения мы рассматриваем как теоретические основы, так и практические реализации, подчеркивая, как эти модели достигают линейной временной сложности, сохраняя при этом сопоставимую производительность с трансформерами на различных задачах моделирования последовательностей.
\end{abstract}

\section{Математические предпосылки}

\subsection{Разложение по базису в векторных пространствах}

Рассмотрим конечномерное векторное пространство $V(\mathbb{R})$ с базисом $B = \{\vec{b}_1, \ldots, \vec{b}_N\}$. Любой вектор $\vec{a} \in V$ может быть выражен как:

$$\vec{a} = c_1\vec{b}_1 + \ldots + c_N\vec{b}_N$$

где $\vec{c} = (c_1, \ldots, c_N)$ — это вектор координат. Если базис ортонормированный:
$$\langle \vec{b}_i, \vec{b}_j \rangle = \begin{cases} 1, & i = j \\ 0, & i \neq j \end{cases}$$

то коэффициенты могут быть вычислены как:
$$c_n = \langle \vec{a}, \vec{b}_n \rangle.$$

При любом базисе векторы координат $\vec c$ образуют собственное векторное пространство $\mathbb{R}^N$. Но если базис ортонормированный, то между $V(\mathbb{R})$ и $\mathbb{R}^N$ есть изометрический изоморфизм. Если допустить некоторую нестрогость интерпретации, то с точки зрения машинного обучения это означает, что вектор координат является идеальным признаковым описанием исходного вектора.

\subsection{Функциональные пространства и ортонормированные базисы}

Те же принципы распространяются на функциональные пространства. Для функций $f, g \in L^2[-1,1]$ мы определяем скалярное произведение как

$$\langle f, g \rangle = \int_{-1}^1 f(x)g(x) w(x) dx,$$

где $w(x)$ — весовая функция. Классическими примерами ортонормированных полиномиальных базисов служат полиномы Лежандра, полиномы Лагера, полиномы Чебышева и полиномы Эрмита.

\begin{table}[h]
\centering
\small
\renewcommand{\arraystretch}{2.5}
\begin{tabular}{m{0.26\linewidth}|>{\centering\arraybackslash}m{0.12\linewidth}|>{\centering\arraybackslash}m{0.15\linewidth}|>{\centering\arraybackslash}m{0.3\linewidth}}
\textbf{Название} & \textbf{Простр.} & \textbf{Весовая ф-я} & \textbf{Явная формула} \\
\hline
Тригонометрический ряд & $L^2[0, 2\pi]$ & $w(x)=1$ & $\displaystyle\{\exp(inx)\}_{i=-\infty}^{+\infty}$ \\
\hline
Полиномы Лежандра & $L^2[-1,1]$ & $w(x)=1$ & $\displaystyle P_n(x)=\frac{1}{2^nn!}\frac{d^n}{dx^n}(x^2-1)^n$ \\
\hline
Полиномы Лагера & $L^2[0,+\infty)$ & $w(x)=e^{-x}$ & $\displaystyle L_n(x)=\frac{e^x}{n!}\frac{d^n}{dx^n}(e^n x^n)$ \\
\hline
Полиномы Чебышева & $L^2[-1,1]$ & $\displaystyle w(x)=\frac{1}{\sqrt{1-x^2}}$ & $\displaystyle T_n(x)=\sum_{j=0}^{\lfloor n/2\rfloor}\binom{n}{2j}(x^2-1)^jx^{n-2j}$ \\
\hline
Полиномы Эрмита & $L^2(-\infty,+\infty)$ & $w(x)=e^{-x^2}$ & $\displaystyle H_n(x)=(-1)^ne^{x^2}\frac{d^n}{dx^n}e^{-x^2}$ \\
\end{tabular}
\caption{Ортонормированные полиномиальные базисы}
\end{table}

\subsection{Полиномиальные проекции}

Особенность пространства Лебега в том, что базис $L^2$ бесконечный. Это означает, что идеальное признаковое описание функции будет бесконечномерным вектором. На первый взгляд, это ставит непреодолимое препятствие для практических применений... Однако мы можем воспользоваться аппроксимациями!

Для любой функции $f(x) \in L^2$ и ортонормированного базиса $\{P_n(x)\}_{n=1}^\infty$ справедлива формула аппроксимации:

$$f(x) = \sum_{n=0}^\infty c_n P_n(x) \approx \sum_{n=0}^N c_n P_n(x)$$

где коэффициенты:
$$c_n = \langle f(x), P_n(x) \rangle = \int_{-1}^1 f(x)P_n(x)w(x)dx$$

В терминах алгебры такая аппроксимация является проекцией пространства $L^2$ на $N$-мерное линейное подпространство, образованное конечным базисом $\{P_n\}_{n=1}^N$. Чем больше $N$, тем точнее аппроксимация. 

\subsection{Обыкновенные дифференциальные уравнения (ОДУ)}

ОДУ описывают динамику системы:
$$\frac{d}{dt}x(t) = f(x(t), t)$$

Простейшим примером ОДУ являются уравнения движения двумерного коптера, которые можно составить пользуясь вторым законом Ньютона классической механики:
$$
\begin{cases}
    {d\over dt} x= v_{x} \\
    {d\over dt} v_x= - \frac{(u_1 + u_2)\sin(\theta)}{m} \\
    {d\over dt} y= v_{y} \\
    {d\over dt} v_y= \frac{(u_1 + u_2)\cos(\theta)}{m} - g\\
    {d\over dt}\theta = \omega \\
    {d\over dt}\omega = \frac{(u_1 - u_2)r}{I}
\end{cases}
$$

Решение ОДУ (численно или аналитически) называется \textbf{интегрированием}.

Простейшую схему численного интегрирования дает метод Эйлера:
$$
  \frac{x(t+dt) - x(t)}{dt} = f(x(t), t),\\
  x_{k+1} = x_k + f(x_k, k)dt
$$

где $dt$ --- т.н. шаг дискретизации. Для интегрирования методом Эйлера достаточно задать начальное условие $x_0$ и запустить рекуррентный пересчет по формуле для $x_{k+1}$. Физически этот процесс можно интерпретировать как покадровую отрисовку движения некоторой динамической системы. Пример с динамикой коптера:
$$
\begin{cases}
  x_{n+1} = x_{n} + v_{n,x}\text{d}t \\
  v_{n+1,x} = v_{n,x} - \frac{(u_1 + u_2)\sin(\theta)}{m}\text{d}t \\
  y_{n+1} = y_{n} + v_{n,y}\text{d}t \\
  v_{n+1,y} = v_{n,y} + \left(\frac{(u_1 + u_2)\cos(\theta)}{m} - g\right)\text{d}t \\
  \theta_{n+1} = \theta_{n} + \omega_{n}\text{d}t \\
  \omega_{n+1} = \omega_{n} + \frac{(u_1 - u_2)r}{I}\text{d}t
\end{cases}
$$

\subsection{Временные ряды}

Одномерным временным рядом (univariate time series) мы будем называть числовую последовательность $x_1,\ldots,x_T$. Число $x_t\in\mathbb{R}$ мы называем наблюдением, или замером (observation) в момент времени $t$ (timestamp). Термин <<наблюдение>> отражает, в каких областях обычно возникают временные ряды: замер температуры воздуха, концентрации вещества и проч.

Многомерным временным рядом (multivariate time series) мы будем называть последовательность векторов $x_1,\ldots,x_T$. В данном случае наблюдением будет целый вектор $x_t\in\mathbb{R}^C$. Он представляет собой замеры сразу нескольких величин. Например, вместе с температурой можно мерить давление и влажность воздуха. Отличие $C$-мерного временного ряда от набора из $C$ одномерных временных рядов в том, что в многомерном ряде обычно предполагают, что компоненты вектора $x_t$ относятся к одному моменту времени.

\subsection{Рекуррентные нейронные сети}

\begin{wrapfigure}[4]{r}{0.3\textwidth}
\vspace{-1.5\baselineskip}
\centering
\includegraphics[width=\linewidth]{figures/rnn-cell.drawio.pdf}
\end{wrapfigure}

Простейшая рекуррентная сеть задается следующими формулами:
\begin{align*}
\text{RNN}(h,x)&=(1-g)\circ h + g\circ\tanh(W_1h+U_1x+b_1),\\
g&=\sigma(W_2h+U_2x+b_2)
\end{align*}

где:
\begin{itemize}
\item $h \in \mathbb{R}^H$ — скрытое состояние (hidden state),
\item $x \in \mathbb{R}^C$ — входной вектор (текущее наблюдение),
\item $g \in \mathbb{R}^H$ — gate vector, управляющий обновлением скрытого состояния,
\item $W_1, W_2 \in \mathbb{R}^{H \times H}$ — матрицы весов для преобразования ,скрытого состояния,
\item $U_1, U_2 \in \mathbb{R}^{H \times C}$ — матрицы весов для преобразования входа,
\item $b_1, b_2 \in \mathbb{R}^H$ — векторы смещения (bias),
\item $\circ$ — поэлементное умножение (произведение Адамара),
\item $\sigma(\cdot)$ — сигмоидная функция активации $\sigma(z) = 1/(1+e^{-z})$,
\item $\tanh(\cdot)$ — гиперболический тангенс.
\end{itemize}

\section{HiPPO: Рекуррентная память с оптимальными полиномиальными проекциями}

\subsection{Фреймворк}

\begin{wrapfigure}[15]{r}{0.5\textwidth}
\vspace{-1\baselineskip}
\centering
\includegraphics[width=1\linewidth]{figures/continuous-function-1-02.drawio.pdf}
\caption{Идея HiPPO: аппроксимировать функцию на отрезке с помощью полиномиальной проекции.}
\label{fig:continuous-function}
\end{wrapfigure}

Сделаем предположение: пусть за всяким одномерным временным рядом $\{x_i\}_{i=1}^T$ стоит некоторая непрерывная функция $f(t)$, определенная при $t\in[0,+\infty)$. Временной ряд при этом будем называть дискретизацией функции $f(t)$ по некоторой сетке значений $t$. Основная идея HiPPO~\cite{gu2020hipporecurrentmemoryoptimal} в том, чтобы для временного ряда $\{x_i\}_{i=1}^T$ получить коэффициенты $\vec c\in\mathbb{R}^N$ полиномиальной проекции функции $f(t)$ --- тогда эти коэффициенты можно использовать в ML приложениях как признаковое описание временного ряда. Проблема лишь в том, что у нас нет доступа к $f(t)$, мы видим лишь дискретизацию $\{x_i\}_{i=1}^N$.

Пусть $\vec c(\tau)\in\mathbb{R}^N$ --- вектор коэффициентов полиномиальной проекции функции $f(t)\big|_{[0,\tau]}$ (рисунок \ref{fig:continuous-function}). Оказывается, что векторная функция $\vec c(\tau)$ описывается ОДУ:

$$\frac{d}{dt}\vec c(t) = Ac(t) + Bf(t)$$

где $A\in\mathbb{R}^{N\times N}$ и $B\in\mathbb{R}^N$ --- некоторые константы, определяемые выбранным полиномиальным базисом.

Применяя численное интегрирование методом Эйлера:
$$c_{k+1} = c_k + (Ac_k + Bf_k)dt = (I + Adt)c_k + (Bdt)f_k$$

Обозначая $\bar{A} = I + Adt$ и $\bar{B} = Bdt$, получаем:
$$c_{k+1} = \bar{A}c_{k-1} + \bar{B}f_k$$

В итоговом выражении участвует величина $f_k=x_k$ --- один отсчет временного ряда. Таким образом, для вычисления признакового описания временного ряда, достаточно произвести пересчет по рекуррентным формулам для $c_{k+1}$. В этом и заключается HiPPO~\cite{gu2020hipporecurrentmemoryoptimal} --- High Order Polynomial Projections. Этот метод стремится решать задачи на временных рядах с помощью рекуррентной памяти, используя коэффициенты разложения как скрытые состояния. Они служат математическим обоснованием оптимальности сжатия исторической информации.

\subsection{Реализации HiPPO}

В зависимости от выбранного базиса, мы получаем разные $\bar A$ и $\bar B$. Конкретные значения выводятся аналитически. Ниже приведены значения, которые получили авторы HiPPO~\cite{gu2020hipporecurrentmemoryoptimal} (таблица \ref{tab:hippo-impl}). Заметим, что выбор базиса влияет на то, как модель использует предыдущие наблюдения $f(t)$ (рисунок \ref{fig:weight-function}).

\begin{table}[!htb]
\centering
\renewcommand{\arraystretch}{2.0}
\begin{tabular}{l|p{2.5cm}|p{4.5cm}|p{7cm}}
\textbf{Метод} & \textbf{Простр-во} & \textbf{Весовая функция} & \textbf{Элементы матрицы} \\
\hline
\textbf{LegT} & $L^2[\tau-\theta, \tau]$ & $w(t) = \frac{1}{\theta}[\tau-\theta \leq t \leq \tau]$ & $A_{nk} = \frac{1}{\theta}\begin{cases}
  (-1)^{n-k}(2n+1), & n \geq k \\
  2n+1, & n \leq k
  \end{cases}$ \\
\hline
\textbf{LagT} & $L^2[-\infty, \tau]$ & $w(t) = \exp(t-\tau)[t \leq \tau]$ & $A_{nk} = \begin{cases}
  1, & n \geq k \\
  0, & n < k
  \end{cases}$ \\
\hline
\textbf{LegS} & $L^2[0, \tau]$ & $w(t) = \frac{1}{\tau}[0 \leq t \leq \tau]$ & $A_{nk} = -\frac{1}{\tau}\begin{cases}
  \sqrt{(2n+1)(2k+1)}, & n > k \\
  n+1, & n = k \\
  0, & n < k
  \end{cases}$ \\
\end{tabular}
\renewcommand{\arraystretch}{1.0}
\caption{Реализации HiPPO: Translated Legendre (LegT), Translated Laguerre (LagT) и Scaled Legendre (LegS)}
\label{tab:hippo-impl}
\end{table}

\begin{figure}[!htb]
\centering
\includegraphics[width=\linewidth]{./figures/weight-function.png}
\caption{Весовые функции и то, как они учитывают историю $f(t)$.}
\label{fig:weight-function}
\end{figure}

\section{От теории к практике}

\subsection{Linear State-Space Layer (LSSL)}

Основываясь на HiPPO, LSSL~\cite{gu2021combiningrecurrentconvolutionalcontinuoustime} трансформирует теоретический фреймворк HiPPO в слой нейронной сети, пригодный для практических приложений.

Авторы определяют отображение входа $\{u_t\,|\,u_t\in\mathbb{R}\}$ в выход $\{y_t\,|\,y_t\in\mathbb{R}^M\}$:
\begin{align*}
  x_t &= \bar{A} x_{t-1} + \bar{B} u_t \\
  y_t &= Cx_t + Du_t
\end{align*}

где:
\begin{itemize}
\item $\bar{A} \in \mathbb{R}^{N \times N}$, $\bar{B} \in \mathbb{R}^N$ инициализируются из HiPPO матриц,
\item $C \in \mathbb{R}^{M \times N}$, $D \in \mathbb{R}^M$ --- обучаемые параметры.
\end{itemize}

В теории оптимального управления подобные системы уравнений называют моделями пространства состояний --- state-space model (SSM).

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{figures/lssl-sequence-mapping.png}
\caption{Визуализация linear state-space layer (LSSL) как рекуррентной сети.}
\end{figure}

Используя SSM, авторы построили LSSL~\cite{gu2021combiningrecurrentconvolutionalcontinuoustime} --- Linear State-Space Layer (рисунок \ref{fig:lssl}).

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{figures/lssl.drawio.pdf}
\caption{Архитектура linear state-space layer.}
\label{fig:lssl}
\end{figure}

Авторы~\cite{gu2021combiningrecurrentconvolutionalcontinuoustime} стремились реализовать компоненты, напоминающие компоненты трансформерного слоя: add \& norm, коммуникация между токенами, position-wise обработка.


\textbf{Рекуррентность как свертка}

Один из минусов рекуррентной архитектуры в сравнении с трансформерами --- это невозможность параллелизовать обучение. Наличие цикла по $t$ обеспечивает быстрый инференс, например, для задачи next token prediction, но ужасно неэффективно, когда последовательность уже известна на перед, как на обучении. Однако у LSSL есть теоретический способ для ускорения.

Если развернуть рекуррентную формулу, то мы увидим что SSM реализуют свертку:
$$y_t = C(\bar{A})^t\bar{B}u_0 + C(\bar{A})^{t-1}\bar{B}u_1 + \ldots + C\bar{B}u_t+Du_t$$

Это может быть записано как:
$$y = \mathcal{K}_L(\bar{A}, \bar{B}, C) * u + Du$$

где ядро:
$$\mathcal{K}_L(A, B, C) = (CB, CAB, \ldots, CA^{L-1}B) \in \mathbb{R}^{M \times L}$$

Это ядро свертки отличается от того, что мы привыкли видеть в DL. Обычно мы имеем дело с ядрами фиксированного размера, такими как $3\times 3$ (в компьютерном зрении) или одномерные ядра (в текстовых CNN-классификаторах). Тут же ядро $\mathcal{K}$ имеет длину, равную длине входной последовательности.

Привычные ядра фиксированного размера хорошо параллелятся за счет сведения к матричным операциям. Ядра наподобие $\mathcal{K}$ параллелятся за счет использования FFT convolve (Fast Fourier Transform, алгоритм "бабочка"). Тогда обучение выполняется за $O(L \log L)$. А на инференсе у нас есть выбор: либо последовательно за $O(L)$ для задач в духе next token prediction, либо $O(L\log L)$ для задач в духе классификации.

\subsection{Стратегии параметризации}

Выше упоминалось, что матрицу $A$ можно обучать. Согласно экспериментам оригинальной статьи~\cite{gu2021combiningrecurrentconvolutionalcontinuoustime}, это привносит ожидаемую специализацию под датасеты и дает небольшой прирост качества. Однако если делать матрицу $A$ обучаемой, то мы обязаны пересчитывать ядро $\mathcal{K}$ после каждого шага оптимизации, в то время как без обучения $A$ достаточно предпосчитать $\mathcal{K}$ один раз.

Чтобы решить проблему накладных расходов, связанных с высчитыванием $\mathcal{K}$, авторы применили трюк с репараметризацией~\cite{gu2021combiningrecurrentconvolutionalcontinuoustime}. Идея в том, чтобы ограничить $A$ некоторым классом матриц, в котором находятся оригинальные матрицы HiPPO.

\textbf{Трехдиагональная параметризация (LSSL)}

Матрицы HiPPO могут быть представлены как:
$$A = P(D + T^{-1})Q$$

где $D$, $P$, $Q$ — диагональные, а $T$ — трехдиагональная. Это сокращает параметры с $N^2$ до $6N$, сохраняя теоретические свойства. Более того, подсчет ядра с таким $A$ становится быстрее, если правильно определить порядок матрично-векторных операций.

\textbf{Normal Plus Low-Rank (S4)}

Одна из последующих работ (Structured State-Space Sequence Models, S4)~\cite{gu2022efficientlymodelinglongsequences} использует параметризацию:
$$A = V\Lambda V^* - PQ^*$$

где $\Lambda$ — диагональная, $V$ — унитарная, а $P, Q \in \mathbb{R}^{N \times r}$ — матрицы низкого ранга. Это позволяет эффективное вычисление ядра через специализированные алгоритмы.

\textbf{Диагональная параметризация (DSS, S4D)}

Самый простой подход~\cite{gupta2022diagonalstatespaceseffective,gu2022parameterizationinitializationdiagonalstate} использует диагональные матрицы:
$$A = \text{diag}(\lambda_1, \ldots, \lambda_N)$$

Это дает чрезвычайно эффективное вычисление ядра:
If $A=V\Lambda V^{-1}$, then $\exists w\in\mathbb{C}^N$:
$$
\mathcal{K}_L(\bar A, \bar B, C)\Leftrightarrow \mathcal{K}_L(\Lambda, (e^{L\lambda_i dt}-1)_{i=1}^N, w)=w\Lambda^{-1}\text{softmax}(P),
$$
where $P\in\mathbb{C}^{N\times L}$ such that $P_{ij}=\lambda_ij\cdot dt$.

\section{Mamba: Селективные модели пространства состояний}

\subsection{Мотивация}

Хотя SSM показали перспективность, они страдали от фундаментального ограничения: \textbf{инвариантности по времени}. Параметры $A$, $B$, $C$ оставались постоянными независимо от входа, что затрудняло селективное запоминание или забывание информации на основе контекста.

Рассмотрим задачу, где модель должна селективно копировать токены на основе контекста (рисунок \ref{fig:synthetic-copy}). Традиционные SSM испытывают трудности, поскольку не могут адаптировать свой механизм памяти к содержимому входа. 

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{figures/selective-copying.png}
\caption{Синтетические задачи языкового моделирования: копирование.}
\label{fig:synthetic-copy}
\end{figure}

Подобные синтетические задачи созданы для того, чтобы изолированно проверять способности моделей, которые исследователям кажутся ключевыми. Аналогичная ситуация с другими двумя синтетическими задачами: inductive heads и associative recall (рисунок \ref{fig:synthetic-head}).

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/synthetic-lm-tasks.png}
\caption{Синтетические задачи языкового моделирования: копирование.}
\label{fig:synthetic-head}
\end{figure}

\subsection{Linear Attention}

\begin{wrapfigure}[10]{r}{0.15\textwidth}
  \vspace{-1\baselineskip}
  \centering
  \includegraphics[width=1\linewidth]{figures/h3-layer.png}
  \caption{H3.}
  \label{fig:h3}
\end{wrapfigure}

До Mamba, Linear Attention~\cite{katharopoulos2020transformersrnnsfastautoregressive} показал, как эффективно аппроксимировать внимание трансформера:

Механизм внимания $\text{softmax}(QK^T)V$ можно расписать следующим образом:
$$
O_i=\sum_{j=1}^i\underbrace{\frac{\text{sim}(Q_i,K_j)}{\sum_{t=1}^i\text{sim}(Q_i,K_t)}}_{\alpha_{ij}}V_j=\frac{\sum_{j=1}^i\text{sim}(Q_i,K_j)V_j}{\sum_{t=1}^i\text{sim}(Q_i,K_t)}
$$
где $\text{sim}(q,k)=\text{exp}(q^Tk)$. Если выбрать $\text{sim}(q,k)=\phi(q)^T\phi(k)$ с некоторой нелинейной функцией $\phi:\mathbb{R}^d\to\mathbb{R}^d$, то
$$
O_i=\frac{\sum_{j=1}^i\phi(Q_i)^T\phi(K_j)V_j}{\sum_{t=1}^i\phi(Q_i)^T\phi(K_t)}=\left[\frac{\phi(Q_i)^T\sum_{j=1}^i\phi(K_j)V_j^T}{\phi(Q_i)^T\sum_{j=1}^i\phi(K_j)}\right]^T
$$

Определим величины $S_i,Z_i$:
$$
O_i^T=\frac{\phi(Q_i)^T\overbrace{\sum_{j=1}^i\phi(K_j)V_j^T}^{S_i}}{\phi(Q_i)^T\underbrace{\sum_{j=1}^i\phi(K_j)}_{Z_i}}=\frac{\phi(Q_i)^TS_i}{\phi(Q_i)^TZ_i}
\Rightarrow 
\begin{cases}
  S_i&=S_{i-1}+\phi(K_i)V_i^T,\\
  Z_i&=Z_{i-1}+\phi(K_i).
\end{cases}
$$

Получили рекуррентный инференс.

\subsection{Hungry Hungry Hippos (H3)}

H3~\cite{fu2023hungryhungryhipposlanguage} связал SSM и языковое моделирование, аппроксимируя внимание компонентами SSM:

$$Q \circ \text{SSM}_{\text{diag}}(\text{SSM}_{\text{shift}}(K) \circ V)$$

Эта архитектура аппроксимирует внимание трансформера средствами SSM (рисунок \ref{fig:h3}). Для эффективности авторы опираются на Flash Convolution для правильной утилизации GPU.

\subsection{Селективные модели пространства состояний}

Mamba~\cite{gu2024mambalineartimesequencemodeling} вводит входозависимые параметры (рисунок \ref{fig:mamba-algo}):

\begin{align*}
x_t &= \bar{A}(x_t) x_{t-1} + \bar{B}(x_t) u_t \\
y_t &= C(x_t) x_t + D(x_t) u_t
\end{align*}

где $A$, $B$, $C$, $D$ теперь являются функциями от входа $x_t$.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{figures/mamba-algo.png}
\caption{Алгоритм SSM для Mamba.}
\label{fig:mamba-algo}
\end{figure}

Параметры $A$, $B$, $C$ задаются простыми линейными проекциями от входа; вводится селективный механизм, управляющий потоком информации; реализация опирается на кастомные CUDA‑ядра, учитывающие иерархию памяти (рисунок \ref{fig:mamba-cuda}).

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{figures/mamba-memory-io.png}
\caption{Утилизация иерархии памяти в CUDA-реализации Mamba.}
\label{fig:mamba-cuda}
\end{figure}

Поскольку свёртка в явном виде недоступна, для распараллеливания рекуррентности используется алгоритм parallel scan \cite{smith2023simplifiedstatespacelayers}, который обычно в алгоритмах используют для вычисления кумулятивной суммы.

\subsection{Архитектура Mamba}

Блок Mamba включает линейную проекцию входа к скрытой размерности, селективный SSM (входозависимое обновление состояний), Gated MLP с современными активациями (SiLU/Swish \cite{elfwing2017sigmoidweightedlinearunitsneural, shazeer2020gluvariantsimprovetransformer}) и заключительную проекцию обратно к исходной размерности (рисунок \ref{fig:mamba-architecture}).

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{figures/mamba-architecture.png}
\caption{Архитектура Mamba.}
\label{fig:mamba-architecture}
\end{figure}

\subsection{Сравнение с трансформерами}

К сожалению, из-за своей рекуррентной природы Mamba~\cite{gu2024mambalineartimesequencemodeling} все равно плохо справляется с задачами, требующими точного запоминания контекста. Mamba обеспечивает линейно‑временной инференс, хорошо экстраполирует длину входной последовательности и демонстрирует сопоставимую перплексию (рисунок \ref{fig:rejected-mamba}). Однако Mamba слабо поддается in‑context learning, и страдает от проблемы нечеткой памяти (рисунок \ref{fig:mamba-fuzzy}).

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{figures/pure-mamba-synthetic.png}
\caption{Проблема нечеткой памяти в Mamba (fuzzy memory).}
\label{fig:mamba-fuzzy}
\end{figure}

Комбинирование Mamba с трансформером даёт «лучшее из двух миров»: эффективность Mamba сочетается с возможностями внимания, что обеспечивает лучшее качество (рисунок \ref{fig:hybrid-mamba}).

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{figures/hybrid-mamba-extrapolation.png}
\caption{Проблема нечеткой памяти (fuzzy memory) в Mamba решается в гибридной архитектуре, которая чередует Mamba слои со слоями трансформера.}
\label{fig:hybrid-mamba}
\end{figure}

В данный материал не включен рассказ про Mamba-3 \cite{anonymous2025mamba}.

\printbibliography

\appendix

\begin{figure}[!htb]
\centering
\includegraphics[width=0.9\textwidth]{figures/denied-mamba.png}
\caption{Результаты основного эксперимента из оригинальной статьи Mamba~\cite{gu2024mambalineartimesequencemodeling}. Оригинальную статью Mamba реджектнули с ICML 2024. Сможете ли вы предположить, почему? Комментарии рувьюеров тут: \url{https://openreview.net/forum?id=AL1fq05o7H}}
\label{fig:rejected-mamba}
\end{figure}



\end{document}