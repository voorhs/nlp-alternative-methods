Всем привет! Надеюсь экран видно. Всех рад видеть на этом семинаре. Сегодня продолжим рассказывать про state-space модели. Без лишних слов сразу начнем. В прошлый раз я рассказывал про HIPPO. Идея была в следующем: создадим рекуррентную память как коэффициенты разложения по полинолам, то есть функцию на отрезке Просто раскладываем по бачке спаленомов и коэффициенты будут нашим hidden state, нашей регулярной памятью. И получается вот такая функция c. Если функция f непрерывная, то и функция c это такая непрерывная векторная функция. Такая идея. Оказалось, что эта функция c, она описывается некоторой динамикой вектор, дифференциальным уравнением вот таким. И... После дискретизации этого дифференциального уравнения можно получить рекуррентные формулы для пересчета и уже использования этой модели для временных рядов. Временный ряд - это просто последовательность значений. И здесь вот матрицы a и b, они не зависят от функции f, которую мы хотим пройти нашей моделькой. Это вот они зависят только от базиса из полинома, который мы выбрали. И в частности, вот были продемонстрированы такие вот формулы. Так вот выглядят матрицы А, вектор B. В прошлый раз вот что я не сказал, это про разные методы интегрирования дифференциальных уравнений. Я сказал только про метод Эйлера. Он выглядит вот таким образом. У нас есть какое-то дифференциальное уравнение. мы его интегрируем как вот аналогично градиентному спуску предыдущее значение плюс маленький шаг по градиенту и вот давайте введём обозначение c вынесем за скобку f вынесем за скобку и вот эти матрицы в скобках обозначим как вот а с чертой и b с чертой и будем называть это как бы дискретизированные версии исходных матриц а которые вообще-то получаются с помощью всяких теоретических выводок И вот именно такой вид у этих матриц определяется методом интегрирования, методом Эйлера. Вообще есть методов много. Вот, например, есть обратный метод Эйлера. Тогда матрица будет выглядеть таким образом. Есть билининный метод, он более точный, подходит для других случаев, нежели методы Эйлера. Выглядит так. Есть еще ZeroOrderOld метод, даже не знаю, как это по-русски будет. Тут возникает матричная экспонента, Это вот больше подходит под линейные системы, как у нас. Сказал я это про то, что вот если мы просто берем метод интегрирования, получаем выражения для матрица i, b, подставляем их вот в эту формулу, и вот у нас есть recurrent и inference. Это вот все результаты предыдущей статьи. И вот в предыдущей статье предлагалось использовать эту штуку, эту рекуррентную память, совместно с RNNG. Как дополнительный вход к основному входу конкретинируется вот этот вектор коэффициентов разложения, который запоминает историю функции f. Это некоторая взвешенная сумма hidden states. Считается, что это очень важно, запомнить эту функцию. Мы предполагаем, такая у нас гипотеза. И вот очень помогает... В общем, вот интеграция такая странная использовалась в предыдущей статье. То есть это не полноценный слой какой-то. Собственно, вот следующая, первая статья, которую я сегодня собирался разобрать, LSSL, это вот Linear State Space Layer. Это уже про то, как из HIP сделать полноценный слой. Это вот уже называется State Space Model. И, кстати, вот в прошлый раз я еще забыл сказать, почему это вообще называется таким странным словом State Space Model. А дело в том, что вот это вот дифференциальное уравнение, вот эта рекуррентная формула, она в теории управления называется вот State-Space Control. Потому что это вектор C, он как бы представляет состояние нашей системы, там всякие векторы скорости, ускорений, физическая какая-то система. И вот State-Space, когда мы интегрируем это уравнение, мы получаем некоторую траекторию в пространстве состояний. А Linear, потому что здесь вот линейные операции используются. В общем, вот Linear и State-Space Layer. Можно еще к вопросу к предыдущему слайду? У нас получается две функции, это CT и HT. И вот HT, он отображается еще в HIP, да? Да. То есть мы этот вектор коэффициентов не используем как полноценный такой вектор контекста, hidden state. Он такой вспомогательный, основной все-таки, такой же, как и в эронавике. Вот. Так, собственно, как сделать из этой идеи, из этой теории полноценный слой? Давайте рассмотрим такую ситуацию. Вот у нас есть входная последовательность какая-то, одномерная, вот U1, U2 и так далее. И мы что делаем? Мы применяем вот к ней, на место F, мы применяем, поставляем это U, и теперь, в общем, применяем вот эту нашу динамику HIPAA. x будет нашим hidden state, это будут наши коэффициенты разложения. Мы как бы запоминаем этот временной ряд с помощью наших разложений по полиному. А затем вот эти коэффициенты, этот hidden state, мы отображаем с помощью матрицы c и d в уже выход из этой ячейки y. То есть x это вектор размера n, а c это матрица m на n. Получается, что у него m компонент. И получается, что вот эти два уравнения вместе уже можно осматривать как некоторый полноценный слой, который переводит одномерную последовательность U в M-мерную последовательность M. Вот это будет называться у нас SSM - State Space Model. И вот C и D - это будут некоторые обучаемые параметры. А и Б это некоторые матрицы, которые мы получили с помощью хипа. Это хипа матрицы фиксированные. Их один раз теоретически мы вывели и поставили сюда. Их можно обучать в теории. Но мы поговорим о том, как сделать это эффективно попозже. Сейчас надо поговорить про то, что на самом деле вот это уравнение, эти рекордентные уравнения, они реализуют некоторую свертку. Повнимательно посмотрим. Вот формула для выхода. В общем, раскрутим эту рекуррентную формулу, сюда поставим определение для x, t. Это вот такая штука. x, t выражается через x, t минус 1. Потом для x, t тоже поставим эту рекуррентную формулу, и получается так. Вот так до конца все сделаем, и получим вот такое выражение, сумма значений. Здесь вот u0, u1, ut-1, они все умножены на такое выражение c, a, b, c, a, b, c, a, b, где a в какой-то степени. B это вектор, C это матрица. То есть вот здесь вот, если мы C обоимножим, то это будет вектор с m компонентами. И, в общем, оказывается, что здесь вот везде C, A, B — это вектор с m-компонентами. То есть мы вектор умножаем на какой-то U0, вектор на U1 и так далее. В общем, оказывается, что все это выражение — это некоторая свертка. Это даже видно. Это свертка исходной последовательности U с некоторым ядром, которое записано вот таким образом. C, B, C, A, B и так далее. Причем вот что тут необычно, это как бы не как в торче conf2d, conf1d, где ядро какого-то фиксированного размера, там 3х3 и так далее. Здесь ядро имеет длину такую же, что и входная последовательность. И здесь ядро, оно как бы конструируется из матриц C, B. То есть это некоторое структурированное такое ядро. И получается, что все это ядро, это как бы ядро размера L с M фильтрами. Вот так вот неожиданно получается, что вот эти две рекуррентные формулы, они просто реализуют некоторую свертку. И чем это круто? Круто это тем, что свертка, она хорошо параллелится на GPU. И то есть вот есть вот эта знаменитая проблема, чем хороши трансформеры и чем плохие RNN-ки, что трансформеры, они параллелят свое обучение очень легко, потому что там все сводится к матричному векторному произведению, Это хорошо тренируется. Параллельно с RNN, из-за того, что там рекуррентная формула, это никак в теории не спараллелить. А здесь мы вот смогли нашу рекуррентность представить в виде свертки, а свертка хорошо параллелится. Вот, собственно, этот слайд. И мы можем как бы совместить вот эти два плюса, что мы можем, используя рекуррентные формулы, иметь легкий inference, зависящее от длины генерируемой последовательности, А train мы можем распараллелить. Причем, если мы просто распараллелим свертку, то это будет O . Но мы можем еще дополнительно ее соптимизировать, если мы будем использовать не обычную свертку, а свертку с быстрым разложением Fourier. Fast Fourier Transform. Там сложный алгоритм, я изначально хотел его рассказать, это интересно, но передумал. Алгоритм называется бабочка. Как там круто посчитать быстрое произвольное Fourier. Еще это применяется для сверток. Собственно, есть целые пакеты для GPU-шек, типа QFFT, QDFT, в которых все это реализовано. То есть что мы имеем? Мы имеем очень крутую операцию, которая быстро реализуется, которая даже не квадратична, еще и которая параллелится. Например, бучки. Это один из главных трюков этой статьи. И все вместе получается вот такой слой. Вот один рекордентный блок LSSL. Значит, у нас есть на входе какой-то вектор, активация какая-то промежуточная, или вход, у которого AD-компонент. Каждую из этих компонент мы обрабатываем с помощью нашего HIP-фреймворка, поскольку он работает только с одномерными временными рядами, мы не можем сразу весь вектор. Поэтому мы все компоненты отдельно обрабатываем, и для каждой компоненты у нас вот свой hidden state получается. D hidden states. Это как раз-таки коэффициент разложения на базе спаленомов. Потом каждый из этих hidden state мы отображаем в выход, то есть он уже имеет размер m, потому что матрица c имеет такой размер. А затем просто все это слопаем обратно в размерность d, чтобы на входе и на выходе была одинаковая последовательность с помощью полноценного слоя MLP. Вон там только один слой используется. В конце еще... Извини, можно? Блин, да извини, что прервал. Я хотел в конце вопрос задать, но сейчас прям совсем непонятно. У нас на каждой... Вот UT это одномерная? Ты говорил просто, что это одномерная штука, но это d-мерная, судя по всему. У t - да. И мы каждую отдельную компоненту d и t раскладываем на n, где n - это количество базисов нашей функции разложения. Да, количество полиномовых базисов. То есть мы каждую компоненту этого вектора рассматриваем как элемент отдельного временного ряда и все их независимо обрабатываем. Если бы здесь D было 1, то все это было бы обычной хиппо. Мы бы обобщили хиппо на случай демерных входов. И тут вот все как бы независимо происходит, все компоненты обрабатываются независимо, и отчасти для этого, для того чтобы все равно какой-то feature interaction был, вот тут добавлен MLP, потому что в нем как раз все вот эти разные компоненты, они как-то смешиваются. Под или нет MLP, между этими слоями MLP. В общем, такая идея. Под... И тут вот c и d обучаемые параметры, у MLP есть обучаемые параметры, а их b можно обучать, можно не обучать, но об этом попозже. В общем-то вот такой слой. Можно сразу вопрос, просто там есть сильная противоречия, на мой взгляд, с тем, что ты до этого сказал, я просто что-то не понимаю. h, большое t-1, это результат с предыдущего итерации, это рекуррентная часть, верно? Да. А как нам распараллелить эту часть? Пока не звучит, так как будет параллельно. Потому что до этого нужно блок посчитать на u, t-1, да? Как распараллелить? Во-первых, тут два уровня параллелизации. Первый - это по этим компонентам, что каждую компоненту мы независимо обрабатываем. А вторая параллелизация - это вот эти вот формулы. Они с помощью сверток считаются. Поскольку во время обучения мы всю последовательность знаем наперед, поэтому можем просто свертку это сделать. Я ответил на вопрос? Меня спущает кружок H T-1. Откуда он приходит? Он приходит из U T-1. от текущего входа у нас всегда на выходе есть вот некоторые hidden state, ht. Ну когда мы ut уже посчитаем, ut -1 когда уже посчитаем. А таких t будет k длины последовательности, правильно? Да. tk длина последовательности. То есть на самом деле сначала считаем один блок, второй блок и эта линия. Да, это в рекуррентном режиме работает. Рекуррентная линия, да, окей, хорошо. Вот. Так, дальше. Значит, вот такой слой. И сейчас поговорим про то, как обучать матрицу А. Про Б не говорится ничего. Как обучать матрицу А? Она здесь самая главная. Она определяет динамику, как изменяется у нас hidden state. Мы могли бы ее зафиксировать, использовать эти HIPPO-матрицы, которые мы аналитически вывели. Аналитически лучшая матрица для того, чтобы совершать оптимальные проекции на базе спаленомов. но всегда хочется что-то обучить мы дельщики мальчики лучше там подогнаться и все такое в общем если обучать а наивно то есть просто взять там рандомно инициализировать а матрицу нн и обучать это будет очень неэффективно это будет очень глупо поэтому есть вот такой крутой прием с репараметризацией. Оказывается, все эти HIPAA-матрицы представимы вот в таком виде: P, D и Q - это диагональные матрицы, а T - это 3-диагональная матрица. То есть у каждой матрицы HIPAA можно найти такие P, D, D и Q для них. И фишка какая: мы берем HIPAA-матрицы, одну такую HIPAA-матрицу, мы находим для нее вот такое представление, pd.eq и используем это как инициализацию. А затем эти pd.eq обучаем. То есть мы стартуем из хипа и обучаем, как там нам нужно. И вот такая идея. Во-первых, она очень умная. То есть мы обучаемся не просто какую-то матрицу А. Мы обучаем матрицу А в том же классе матриц, что и наши теоретически выведенные оптимальные матрицы. Во-первых. А во-вторых, это еще и типа сокращает число параметров. Потому что так было бы n квадрат, а здесь всего 6n. Типа раз, два, три и еще три. Три диагонали матрицы. Вот. И это дает не сильный прирост. Вот тут маленькая таблица. Они много облаишенов проводили. Обучать или не обучать матрицу. Это дает не сильный прирост, но это везде дает прирост. Маленький, но consistent прирост. Вот. И на самом деле все следующие статьи Ладно, про следующую статью. Вот эксперименты. Какие эксперименты они делали с такой архитектурой? Вот Sequencing Managed Classification это было в прошлом разборе, не буду повторяться. Они вот два таких эксперимента провели интересных. Первый это работа с аудио. Аудио это тоже временный вариант. И они замерили, как быстро их модель работает. В общем, аудио. Расскажу сначала постановку проблемы, задачи. Что обычно, как поступают с аудиосигналами в глубоком обучении. Вот сам сырой аудиосигнал, это вот просто колебания такие. Вот есть частота сэмплирования там 16 тысяч, там 32 тысячи, 48 тысяч. Чем больше частота, тем выше качество аудио. Но не только. Это не основной параметр. Не единственный параметр, точнее. И получается, там в секунду у нас есть 16 тысяч отчетов. То есть 1 секунда аудио – это уже очень большой временной ряд. Обычные модели с этим не справляются. Поэтому обычно применяют быстрое производение в FURIE и получают спектрограммы. То есть раскладывают частотный спектр, и там окнами проходят по этому сигналу. И получают последовательность, которая там уже не десятки тысяч, а сотни. Ну или там тысячи. То есть сильно сокращается именно длина временного ряда. А вот они что сделали? Они провели эксперимент над типа сырым аудиосигналом. Вот такая немного искусственная задача, но интересно. Односекундные спич-команды. Классификация команд. Какое слово там было произнесено, какая команда. И вот это у них все равно сработало. Они там не будут приводить результаты, но, в общем, там всех рвут и мечут, несмотря на то, что у них рекуррентная архитектура работает с длинными последователями. Всегда же у нас было знание такое, что RNN-ки, они очень плохо работают с длинными последователями. Transformer намного лучше. Но вот они сделали такую рекуррентную архитектуру, которая даже такие последовательности умеет обрабатывать. и вторых за меня дошло дошло где-то минут пять назад что все коэффициенты которые были на предыдущих слайдах у тебя аж ты минус один помечены они на самом деле там учитываются это хоть и рекуррентно но это все равно уходит в свертку просто перемножается за раз то есть это хоть и рекуррентно но По сути, оно параллелится, главное, с нужными коэффициентами каждой из U-тетых. Ну, или что-то там еще используется. H-тета. Идея такая. Вот. Еще один эксперимент про то, насколько эффективна у них модель получилась именно с точки зрения обучения, насколько быстро обучается. В общем, они что взяли? Они взяли три датасета, на которых они побили соту. и сравнили с предыдущими сотами на этом датасете в плане обучения. Значит, вот у нас датасет Permuted Mist, это Sequence Image Classification, Time Series Classification, то же самое. И вот они, вот предыдущая сота, это вот эта вот моделька, что-то Continuous Kernel Conf, когда DACа расшифровывается. И вот, чтобы получить 98% аккуруси, ему понадобилось 118 эпох, а их модельке всего 16. Ничего себе ускорение. Потом вот соты, которые они достигли, они достигли за 200 эпох, а их моделька достигла за 104 эпохи. А если смотреть не по числу итераций, а по времени, типа, волтайм потраченный, то они вот пятикратный прирост вообще получили. И так вот со всеми датасетами. Здесь вот там больше, чем 10 раз сокращение, здесь вот там 6 раз. В общем... Впечатляюще. Но надо отметить, что здесь вот fixed. Здесь вот матрица А не обучается. Здесь она зафиксирована на HIP. То есть здесь нет погонки. Но при этом все равно соц. достигается и достаточно быстро. Они не объясняли, почему не обучали ее? Вот. Сейчас это самое интересное. Сейчас будет. Это главная проблема. В общем, LSSL очень эффективная архитектура в плане алгоритмов. только до тех пор, пока мы не хотим обучать матрицы А и В. Почему? Потому что вот одна, оказывается, есть еще вот такая тяжелая операция - вычисление самого ядра свертки. Потому что вот нужно L раз вот такую штуку посчитать для кат, навода до L-1, на самом деле, опечатка небольшая, L-1. И это вычисление тяжело. Если считать наивно, то это вот столько. L - это размер последовательности, а N - это число коэффициентов полиномов в этих разложениях. Это, типа, очень грустно, что вот такая сложность. Мы же хотим масштабировать модель, а тут квадратичность, это еще и на длину последовательности умножается. В общем, вычисление самого ядра, она очень тяжелая операция. А если вот у нас, как бы, матрица зафиксирована, то мы один раз ядро вычислили, и во время обучения больше, как бы, просто из кэша достаем, и все. А если вот мы каждый раз, на каждом шаге будем заново это ядро вычислять, вот, чтобы backpropagation, и все такое, еще через него делать, вот, возникает вот такая огромная сложность. И вот это проблема, которую пытаются решить все следующие статьи. Вот S4, DSS, S4D, то статьи, которые сегодня буду еще рассказывать, это они вот получили такой алгоритм, такой квазилинейный, то есть там логарифмические множители отброшены, опущены в этом выражении. И там по памяти тоже очень быстро все. В общем, как они это сделали... как они решили эту проблему числения ядра с помощью более простых параметризаций матрицы А. Значит, вот в LSSL у них была 3-диагональная параметризация, а S4, DSS и 4D они предложили другие. Вот спойлер, самая лучшая модель, самая последняя из 4D использует диагональную аппроксимацию, то есть матрица А это просто диагональная какая-то матрица, и В общем-то, я не буду подробно объяснять все эти параметризации, там очень много математики, я бы хотел всё это рассказать, но это будет трёхчасовой семинар, поэтому просто вкратце, ещё раз идея. Идея вот из этого слайда. Мы хотим обучать матрицу А. Как это сделать? Мы определяем некоторый класс, некоторую параметризацию, инициализируем вот эти все матрицы, матрицами HIPAA, то есть раскладываем HIPAA матрицы, вот такое разложение, и потом это все обучаем. И это получается очень эффективно и по памяти, и по скорости сходимости, потому что мы сразу стартуем из очень классной инициализации, теоретически правильной. И вот просто следующие модельки, следующие статьи, они предлагали вот другие классы. Вот предложив другой класс, более простой какой-то, они получали не только сразу эффективность по параметрам, но еще и они получали, что вот это ядро можно вычислять намного проще. Если вот матрица А диагональная, то это вообще элементарно все обучается. Ну, собственно, вот основная идея всех следующих статей. И я сейчас вот быстренько стараюсь, как смогу рассказать про эти статьи. Значит, вот первая S4, Structured State Space Sequence Model. Structured, потому что вот они задумывались о структуре матрицы А. Значит, какую структуру они выбрали? Они выбрали вот такую параметризацию. это Normal + LowRank Здесь вот это лямбда, диагональная матрица V - это унитарная матрица, то есть это некоторые собственные значения, типа унитарный базис Вот так, типа это общий вид нормальной матрицы А это pq - это какие-то LowRank матрицы, то есть какая-то LowRank корректировка малоранговая И в общем, тут обучаемым параметром являются только лямбда и pq Вот, V - это один раз там находится и все, не обучается Вот, у нас теперь почти 3M-параметры, лямбда и бэку. И вот если вот в таком виде использовать эту параметризацию, можно придумать очень простые, легкие, точнее, очень эффективные алгоритмы для вычисления ядра. Вот та вот проблема, да, что ядро очень тяжело вычислять наивно. И вот так выглядит этот алгоритм. Он состоит из пяти шагов, и здесь каждый шаг – это какая-то отдельная область математики. На самом деле, я не буду все это объяснять, это очень интересно. И вот с помощью такого алгоритма они получили очень классные результаты на экспериментах. Во-первых, они побили соту на Long Range Arena. Это классификация очень длинных временных рядов. То есть вот как это выглядит. То есть вот какую революцию они совершили в свое время. Вот такой прирост. С помощью вот этой своей модельки S4. Потом они даже немного посмотрели про языковое моделирование на виде текст. Спойлер результата не очень, но на уровне с трансформером, как это они утверждают. Только модели маленькие, смотрели, сильных экспериментов не проводили, то есть с GPT не сравнивались. Но вот зато опять подчеркнули, что они в 60 раз быстрее, чем трансформеры генерируют все эти токены. Но, собственно, вот. То есть еще раз идея, что другую инициализацию берем, придумываем очень классный алгоритм для вычисления ядра, и это нам позволяет еще и матрицу А подгонять. Очень эффективно подгонять матрицу А под DDoS. И это дает нам соту на многих бенчмарках. Вот. Это S4. И, значит, следующая статья – DSS. Значит, они начинают с такой проблемы, что S4 это очень крутая статья, но сам алгоритм очень сложный, difficult to understand, imprimit and analyze. Поэтому нужно как-то упростить. И они придумывают диагональную параметризацию. То есть они диагонализируют матрицу А, обучают только лямбда, И вот у них там опять крутой какой-то алгоритм для обучения ядра получается. Все равно выглядит страшно, но уже не так страшно, как в предыдущий раз. Всего 4 шага, которые уже не являются каждый шаг отдельным разделом математики. Вот. И опять, они получили вот такую, используют вот такую параметризацию, и поэтому они достигли качества такого же, что и было у S4. Или даже лучше. Это причем видно здесь вот, вот DSS. То есть параметризация теоретически хуже, но качество все равно получается неплохое. И S4D, следующая статья, они к чему пришли? А, так, стоп, S4D, DSS, еще DSS, эксперименты. Они провели все же самые эксперименты, что S4, но за счет того, что они упростили модель, они теперь смогут ее интерпретировать каким-то образом. И вот они интерпретировали ее путем визуализации вот этих ядер, вот этих гернала, которые считаются, их можно визуализировать. И они посмотрели, значит, Как выглядят эти ядра для всех задач? Значит, вот эта картинка, это визуализация вот этих ядер, всех ядер, которые есть в модели. То есть все фильтры со всех слоев вот так вот визуализированы. Они, видимо, еще как-то дополнительно ассортировали или нет, или так само выстроилось, вот эти линии. Ну, в общем, что это нам говорит? Здесь вот, значит, светлое это ноль, темное это единица. Вот в этой задаче long-range dependencies практически не используется, потому что здесь все светлое. Используется только предыдущий, свежий контекст. А вот в этой задаче здесь даже больший разброс, то есть здесь в большей степени используются какие-то длинные закономерности, длинные последовательности, очень важно их выучивать. И вот если обучить нашу модельку на эту задачу, то получаем вот такие ядра. И видно сразу, какая задача самая сложная. Самая сложная задача PathX, потому что она требует больше запоминания. Она требует, чтобы этот слой знал, что было тысячу шагов назад. Вот что означает это ядро. Подожди, слой или шаг? Одна строчка – это один слой. И датализировано, какой вес каждому шагу назад, в прошлое смотреть, какой вес каждому прошлому шагу он дает Понятно, хорошо И что там, 1500 слоев? Это же одномерное, один слой это M фильтров, порядка сотни И слоев порядка десятки, и получается 1500 А, я понял А вот тогда еще осталось непонятным, почему у S4 же тоже есть ядра, их можно визуализировать, почему? Ну вот, тяжело, видимо, было им после всех этих вот этих жестких алгоритмов еще и заниматься экспериментами на визуализацию. Просто раз они говорят, вот этот DSS, они говорят, что S4 не интерпретируем, а мы интерпретируем. Приводит свертку. Такие же картинки можно было для S4 нарисовать. Я не понял позиции. Ну, я согласен, согласен, что можно, но просто вот в той статье они не привели такие картинки, а в этой привели. Ну, как мне кажется, да, можно и там сделать, но видно, что здесь это сделать много проще. Много проще выучить это ядро всего четыре шага. Я вот такой пойнт вижу. Я не понял все-таки. Сорян, долго думал, не получилось. Вот смотри, от 0 до 1500, это их кернелы по слоям. Их много? Их много. То есть это не слои? Это что такое 1500? Это один слой, это M кернелов. - Типа kernels m... - m kernels, да? - Ага. - Окей, это m kernels на последовательность длины m, правильно? У нас, по-моему, там такое условие было. - Последовательность длины l. - m kernels на последовательность длины l. - Здесь последовательность 1024 длина. - Ага. - И как бы вот всю предыдущую историю этот kernel умножает на такие веса, которые в одной строчке указаны. - Да-да-да. Просто я, наверное, поясню, что меня смущает. Меня смущает то, что кернелы вроде разные, там даже как-то скачками от слоя к слою меняются, должны меняться, но эти скачки не видны, если я правильно понимаю. Какая-то такая плюс-минус очень аккуратная, особенно вот на ретривал-задаче, там такая аккуратная кривая, хотя вроде кернелы от слоя к слою должны сильно меняться, нет? Я что-то путаю или не так понимаю? Честно, я вот тоже удивлен, что такая тут кривая вообще получается. не очень понятно типа как будто с увеличится углублением и наоборот с углублением мы смотрим все дальше и дальше в основном может быть они как-то ассортировали типа нашли каждой строчке ара максимум и по нему ассортировали чтобы получилось такая прямая вот они тоже все значения должны даже ассортированные значения то что в таком виде тоже Да, вот, естественно, поле для интерпретации, изучения. Почему именно так? То есть вот отдельный ресёрш можно провести, проинтерпретировать кёрновые для разных задач. Потому что они... Может быть, я плохо читал, конечно, но вот я больше не понял. Я понял, что они просто взяли все слои и вот их так построили. Может быть, что-то они тут ещё придумали. Я вообще думал, что это... Я вначале подумал, что это просто разбиение. Сейчас... Хотя нет. Я думал, что это просто такое количество разбиений, полторы тысячи у нас на все слои, на один слой выходят функции, и вот так они смотрят на предыдущий контекст. Примерно с таким, ну как спектр, это очень похоже на эти спектрограммы. Что ты разбиение называешь? Смотри, как все в спектрограммах, у нас бьется на... громко, да? Бьется по определенным окнам весь сигнал. И каждое окно, по ней считается спектр. Правильно? Да. И сверху, точнее, по y у нас частоты, то есть, по сути, коэффициенты призначения функции, которые использовались. Соответственно, по x у нас частоты как раз, ну, токены, да, в которых мы эти спектрограммы кодировали. Вот это очень сильно похоже на спектрограммы и как раз, ну, очень сильно похоже, короче. И на, соответственно, вот эти kernel, они похожи просто как, ну, ладно, как же не считать. Похожи на спектрограммы, но... Очень похожи, да. Похожи, но мне кажется еще, ну, может быть, они реально ассортированы, но как бы это логично, что вот эта кривая, она все значения по горизонтали проходит где-то чуть поменьше там отмер до 200 в ретривере например да паз x побольше это значит что в обычном
