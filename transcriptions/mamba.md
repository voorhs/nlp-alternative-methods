Что ж, рад всех видеть на этом семинаре. Наконец-то сегодня расскажу про Mambu, про то, как удалось применить State Space модели для языкового моделирования. Вот такой план будет рассказа. Я сначала погружу в контекст. дам, так сказать, вайп исходной Mamba, очень большой, там много пререквизитов. Напомню, стоит Space Models, объясню идею Linear Attention, объясню еще одну новую статью, которая была предшественником Mamba и на которую авторы сильно опирались. В общем, надеюсь, будет все понятно и интересно. Итак, первая статья Linear Attention 2020 года. Она не особо популярная, почему-то там немного цитирований, но результат интересный. В ней авторы смогли показать, как можно свести трансформеры к RNN. Идея, значит, следующая. Вспомним наш attention. Что такое attention? Это вот softmax от attention матрицы запроса на ключи и умножить на матрицу значений. Это означает, что вот в этом softmax построчным мы получаем некоторые attention скоры, веса, и с этими весами мы берем строки матрицы V. По-другому это может быть записано так, что один выход attention-а, o и t, это взвешенная сумма строк матрицы V, то есть некоторых векторов, с некоторыми весами. И эти веса - это вот те самые attention-скоры. Альфа и жид обычно обозначаются. Первое, значит, проделаем арифметическую операцию, вынесем знаменатель за скобку, за это суммирование. Тогда у нас получится вот такая формула. В числителе взвешена сумма векторов, а в знаменателе одна константа. И, значит, исходный attention, который трансформер, attention изолгенит, он использует вот следующую функцию близости между ключом и запросом. Экспонент от скалярного произведения. И основная идея авторов linear attention - давайте заменим эту функцию другой. И они предложили вот такую функцию - скалярное произведение от какого-то нелинейного преобразования. Здесь phi - это какое-то фиксированное нелинейное преобразование. Его можно обучать, можно не обучать, но суть в том, чтобы как-то приблизить исходную экспоненту вот таким образом. Что это нам дает? Если поставить вот эту формулу сюда, то мы видим, что φ от запроса можно вынести за скобку и в числителе, и в знаменателе. И получается вот такая формула. Здесь простейшие правила матричной алгебры, еще сложные. И если ввести вот такие обозначения S, IT, Z, IT, то получается, что вычисление одного выхода по ИТОВА это использование вот такой формулы, где S, IT, Z, IT, она вообще пересчитывается по рекуррентным формулам, потому что это вот просто такая штука. И вуаля, мы получили RNN из трансформера, причем RNN, который имитирует исходный attention. Вот, это, тут вот еще раз, идея в том, что мы какую-то часть attention'а заменили приближенным вычислением, вот таким, и это нам открыло возможности для более эффективного вычисления. В данном случае для рекуррентного инференса. Вот, linear attention, это потому что linear time inference, асимптотика. Окей, это вот первая идея, на которую очень сильно опирались авторы Мамбы. В этой статье на самом деле не было никаких экспериментов про языковое моделирование. Вот эту архитектуру linear attention авторы проверяли. Они проверяли, как быстро она сходится, насколько там меньше вычислительных затрат нужно по сравнению с трансформером. Проверяли на задачах авторепрессионного генерации изображений на простеньких датасетах. И применили еще для ISR, как backbone для распознавания речи. И что важно, то есть здесь нигде нет языка. И что еще, отмечу, что они для этой архитектуры сами написали низкоуровневый код, удокернул, 200 строк он занял, для того, чтобы эффективно forward и backward вычислять со всеми градиентами. Дальше. Это была первая часть рассказа. Вторая часть - это я напомню State Space Model. Это то, что мы в прошлый раз разбирали. Вот были четыре такие архитектуры, четыре статьи. Быстренько пробежимся. Значит, что такое SSM? SSM - это отображение. В одной последовательности "у" в другую "у". Отображение вот по таким правилам. У, входная последовательность, участвует в формировании некоторого hidden state, Xt, который вычисляется вот по такой рекуррентной формуле, а выход последовательности, он вычисляется с помощью вот этого hidden state и входа, с помощью матриц C и D. Это очень похоже на рекуррентную архитектуру, особенно если вот нарисовать таким образом, что вот U умножается на B, а еще на D, получается hidden state, hidden state умножается на C. В общем, вот есть вот эти R&N ячейки, Отличие в том, что здесь вот у этой RNN ячейки все операции линейные. Вот тут только умножение на матрицу и сложение. И вот в прошлый раз мы говорили, что на самом деле вот этот Y, если его расписать, вот эти рекуррентные формулы все развернуть, то получается, что это свертка. Свертка некоторой исходной последовательности с ядром, которое записывается вот таким образом. И у этого ядра есть некая регулярная структура. И вот эта двойственность, что эта схема реализует одновременно и рекуррентность, и свертку, она позволяла в прошлый раз достичь эффективного тренинга за счет того, что свертка хорошо параллелится на GPU, и эффективного инференса за то, что у нас рекуррентный инференс, один шаг требует константной памяти, константной операции. Вот, очень круто. И вот то, что мы сейчас обсуждали, это вот один вот такой вот блок, вот эти две строки, это вот SSM-блок, но к нему обязательно там всегда добавлялись MLP и всякие skip-коннекшены по типу трансформеров. Вот, это все было напоминание. Еще напомню, что вот эта матрица А, она была очень важна в всех предыдущих разборах. Она там теоретически выводилась, и мы вот в конце прошлого разбора пришли к тому, что самый простой способ инициализировать А, это вот инициализировать эту матрицу в виде диагональной матрицы, вот по таким формулам, комплексные числа причем. И последние эксперименты, которые ставились над SSM-ками, они все затрагивали в основном временные ряды. Классификацию временных рядов и некоторое прогнозирование временных рядов на несколько шагов вперед. Языковое моделирование SSM-ки касались очень мало, мало было экспериментов, и все эксперименты были связаны только с измерением перплексии. Как хорошо данная модель смогла подогнаться под датасет какой-то. Вот. А нормальных исследований, связанных с downstream задачами, там Superglue и все такое, ничего такого не было. То есть интеллектуальные способности моделей языковых не исследовались. И вот сейчас мы приступаем к первой статье, новой, которую я сегодня расскажу. Называется она Hungry Hungry Hippos. Это вот первая, одна типа из первых успешных попыток использовать SSM для языкового моделирования. Если что, это название, это отсылка на какую-то серию Симпсонов. Там серия называлась Hungry Hungry Homer. Так, значит, с чего начинается эта статья? Hungry Hungry Hippos, H3 сокращенно. Авторы приводят пример. Две синтетические задачи, с которыми плохо справляются SSN-ки, но с которыми хорошо справляются трансформеры. Первая задача Induction Head. Это задача, в которой активно используется память. В чем она состоит? У нас есть некоторый вход, который завершается служебным символом. И задача — вспомнить, достать из контекста, какой символ стоял после этого служебного символа. В данном случае это F. То есть вот прямая задача induction head, то есть перенести головку в нужное место и достать информацию из памяти. вторая задача и назвали сша эти прикол и это уже более сложная задача это задача запоминать пары киев или ключ значения вот данном случае ключ а значение 2 ключ c значение c и в конце вот просится выведи значение для ключа а и здесь вот должен быть выход 2 Вот такие две задачи и вот такой эксперимент. Взять двухслойные архитектуры, и вот трансформеры справляются вообще на ура, отлично с этими задачами. А вот лучшие их state-space модели и какая-то рандомная state-space модель, которую я не знаю, они очень плохо с ним справляются, с этой задачей. Что показывает, что с языком, с такими дискретными задачами нужно отдельно как-то поработать. И идея, давайте вот исследуем, что такого есть хорошего в Attention, чего нет в наших рекуррентных моделях. И они пишут, что трансформер, он может напрямую сравнивать токены и напрямую копировать токены с памяти. Сравнивать он их может благодаря Attention матрице, потому что Attention матрица сравнивает все токены со всеми. И можно копировать токены напрямую с памяти, потому что мы напрямую берем и копируем строки в, то есть сами токены, копируем с некоторым весом. И вот это ключевое отличие, что нужно для attention. И вот они что делают? Они делают вещь очень похожую на linear attention. Они берут оригинальный softmax и приближают вот эту формулу с помощью SSM. Вот если посмотреть на linear attention, что здесь есть? Здесь есть какое-то преобразование ключа и суммирование. И они решили то же самое сделать с помощью SSM-моделей. Вот какое-то преобразование ключа с помощью одной SSM-ки, и вот суммирование с помощью другой SSM-ки. И я не буду подробно объяснять, почему эта формула работает, скорее всего, я сам не понял правильно, но идея в том, что это очень сильно напоминает Attention, и позволяет в какой-то мере аппроксимировать этот исходный Attention, при этом используя вычислительно эффективные SSM-ки. Вот в виде графа это выглядит таким образом, что мы из одного XA получаем три представления Q, K, V и производим вот здесь вот поэлементное умножение произведения Адамара. Вот такая идея. Это одна из идей этой статьи H3, которая Ангри Хан Грихипас. Вторая идея, она связана с очень модной сегодня темой, тема утилизации GPU-у. Fused kernel, Flash attention и прочее. Вот я при подготовке к этому разбору узнал вообще, что это такое, разобрался, вот решил рассказать. Значит, есть такая проблема, что GPU может использоваться неэффективно, простаивать. Откуда она возникает? Давайте рассмотрим пример. Мы хотим реализовать функцию вот такую, x плюс 100 умножить на y в третьей степени. Если бы мы писали на дарче, то мы могли бы написать как-то так. XY – это некоторые тензоры, которые хранятся на GPU. Они хранятся в вот такой зелененькой памяти. HBM – это High Bandwidth Memory. Вот в случае А100 наших, например, это 40 гигабайт, и вот такая скорость операций в этой памяти. Когда мы вызываем метод add, На Tensor X. Что происходит? Tensor X и число 100, оно перемещается в более быструю память, как вот регистры у процессора. Это называется SRAM. Static RAM. Она очень маленькая, поэтому X загружается кусочками, но она очень быстрая. В разы быстрее HBM'а. Окей, значит, по кусочкам загрузили x, сделали операции, и затем вот этот код, он обратно вернет x плюс 100 вот в эту hbmpамять. Следующая операция у нас y возвести в степень 3. Опять y и 3 по кусочкам будет перемещаться в срам, там будут выполняться операции быстро, а затем обратно результат вернется в hbmp. И последняя операция у нас умножение. Вот это x плюс 100 нужно умножить на y в третий. И опять эти два тензора, они будут по кусочкам перемещаться в этот срам и потом обратно. И вот проблема здесь в том, что вот это перемещение между этими двумя слоями, их здесь очень много. И из-за этого GPU простаивает, потому что ждет пересылок. И решение в том, чтобы как-то придумать алгоритм, который может сразу взять кусочек X, взять кусочек Y, одновременно их перенести вот в эту быструю память, все операции с ней сделать и вернуть обратно, чтобы не было нескольких пересылок одних и тех же тензоров. И вот в результате такой идеи, например, родился Flash Attention. Сейчас уже там Flash Attention 3 вышел, но вот самый первый Flash Attention, он выглядит примерно так. Вот все, что зеленым отмечено, оно хранится в HBM, а все, что оранжевым, оно в быстрой памяти. И вот здесь идея. Взять вот, реализовать Attention, все вот эти три матрицы, K, U, W, взять вот у них по кусочку, передать, переместить их в быструю память, там произвести все операции и заполнить этот вот результат, вычисление, этот softmax умноженный на b. Это сделать непросто, надо знать низкий уровень программирования и надо знать алгоритмы. Вот здесь вот используется там специальный алгоритм, который позволяет вычислять softmax в онлайн-режиме. То есть имея softmax на трех числах, получить softmax на четырех числах. В общем, такой тоже рекуррентный алгоритм в каком-то смысле. Но в результате, что это получается? Что вот эти все операции, после которых у нас GPU простаивает, потому что ждет, когда тензор переместится обратно, они все как бы сливаются в одну операцию. Это вот называется Fused Kernel, когда мы все, весь attention, всю последовательность этих операций сливаем в одну операцию, и это вот очень сильно ускоряет. Вот там вот сколько. В 6 раз, я так вижу. Или больше даже. В общем, очень быстро получается считать attention. И сейчас это во всех LLM используется, и в торче уже это реализовано. Там есть отдельная функция, которая позволяет вызвать этот flash attention. Вот. И вот возвращаемся к нашей статье Hungry Hungry Hippos. Они сделали похожую вещь только для свертки. Flash convolution назвали это. Тоже вот просто они жонглировали вот этими кусочками памяти, чтобы стрейтвэд рекуррентный, он вообще хранился только в быстрой памяти, а всякие тензоры вот в основной памяти. Я не разбирался в алгоритмах, но идея такая. Создать алгоритм, который учитывает вот эту иерархию памяти, чтобы GPU не простаивал. И это можно использовать в... в их архитектуре, потому что напомню, у них здесь есть сразу две SSM-ки и во время обучения они требуют свертки, которые можно параллелить, поэтому можно дополнительно ее ускорить с помощью вот таких трюков. Вот это все про H3 архитектуру. Эксперименты, которые были с H3, это уже наконец-то полноценный эксперимент над языковым моделированием, над измерением интеллектуальных способностей языковой модели. SuperGLUE, стандартный бенчмарк, Я не помню, как именно они сравнивались, но вот они сравнивали с двумя моделями такого же размера. Но они не сами обучали GPT-Neo, OPT, они, возможно, датасеты другие вообще использовали, не то что GPT-Neo и OPT. Но как результат они получили, что на своем датасете они обучили эту H3 модель и получили результаты неплохие, такие же практически, как у этих двух open-source моделек под этом бетчмарке. Ну и дополнительно они оценивали скорость. Насколько вот их Flash Convolution быстрее работает. Он работает вот два с половиной раза быстрее, чем Transformer Flash Attention. Вот. Супер круто. Быстрее можно обучать. То есть в два раза быстрее можно обучить. Вот. Это было H3. И теперь, наконец-таки, Mamba. Вот все, что я до этого рассказал. Оно, надеюсь, поможет лучше понять Mamba. Итак, опять. Мамба. Да. Там только обучение в 2-4 раза быстрее, да? Да. А инференсор приютный, поэтому он по дефолту будет быстрее, чем трансформер. А длина зависит, в общем, в последовательности? Да. Хорошо. Понял, спасибо. Так вот, мамба, мотивация. Значит, опять, как вот ваш 3, они говорят. Вот есть некоторые синтетические задачи, на примере которых мы можем понять... что хорошего у трансформера и что плохого у наших SSM-ок. Как бы, H3 крутая моделька, но хочется еще лучше. Или хочется просто по-другому подойти к этой же проблеме. H3 вот это один способ, и вот Mamba предлагает еще один способ, как подойти. Значит, вот есть такие задачи. Задача копинга, ну, копирования. Задача селективного копирования, выборочного копирования. И вот та самая задача induction heads. И, в общем, вот с задачей selective coping и induction heads SSM-ки по дефолту по определению справляются хуже чем трансформеры Почему потому что вот в этих задачах вот если в задаче копирования вот вспомним что такое задача копирования у нас на входе есть какие-то вот токены и просто через 1000 там токенов каких-то других мы должны их обратно вывести вот моделька должна вспомнить что там было вначале вывести их и это как бы безусловно генерация мы всегда одно и то же должны выводить А если вот у нас уже возникает такая проблема, где между нашими токенами, типа между объектами интереса есть какие-то шумовые объекты, модель должна уметь фильтровать и должна понимать, что ей запоминать, что не запоминать, как-то обновлять память. Трансформеры это легко делают, потому что у них никакой компрессии нет, они напрямую смотрят на эти токены, они уже там выбирают лучший. А SSM-ки, они не умеют фильтровать, они просто все запоминают. Почему? Потому что вот эти параметры A, B, C, D, они всегда одинаковые. Как бы мы... То есть нет никакой зависимости от того, что мы только что считали. Надо, чтобы была зависимость. Input dependent, чтобы модель была. Мы как бы учитываем здесь этот вход у У с помощью матриц B и D, вот он учитывается. Но вот, видимо, недостаточно. Что все-таки хотелось бы, чтобы больше была зависимость от того, чтобы было у. Простыми вот такими операциями линейными этого не достичь. Вот, надеюсь, объяснил проблему. И решение этой проблемы, оно элегантное и гениальное. Что если вот эту схему x, y, вычисления рекуррентные заменить следующей схемой? То есть алгоритм исходный, SSM, он А, Б, С использует как некоторые матрицы обучаемые параметры. Они как бы тайм инвариант, они всегда одинаковые, они не зависят от входа, это обучаемые параметры нашей сети. А потом они здесь используются для того, чтобы вычислить вход, выход для данного входа. с помощью их матриц. В общем, идея такая: давайте, хоть это теперь будут не обучаемые параметры, это будут некоторые функции от входа. Вот x - это наш вход, и мы берём и прямо на основе данного x вычисляем, какая должна быть матрица b и какая должна быть матрица c, которые вот фигурируют здесь: c и b. d они опускают, у них нулевая матрица d. Но суть в том, что вот введя такую зависимость от входа, они как бы исправляют эту проблему. Они больше вносят зависимости от входа. И вот, в принципе, это основная идея концептуальная. А можно еще до этого момента? Там была такая особенность, что на C и B накладываются определенные ограничения. Но с момента S4 они вообще забили на всякие ограничения, которые направили на E-NIT. По инициализации, да. Вот главное, чтобы было всегда на матрицу А. Типа когда они пытались и на B, но быстро отказались от этого тоже. А сейчас вот они берут и даже пытаются отказаться от игры, от инициализации на матрицу А. Ну, нет, она даже здесь все равно остается, это все равно параметр, который правильный. Вот, structure матриц. На B и C, короче, ограничений нету, как ты спросил. Да, спасибо. Просто некоторые функции. По-моему, у них там простейшие функции, типа линейные слои. Вот. Вот это концептуальная идея, как вести большие зависимости. Но сразу же возникает проблема. Значит, если у нас матрицы B и C будут зависеть от входа X, то что это означает? Это означает, что вот если вернуться на этот слайд, это означает, что вот ядро, оно уже выглядит не таким образом. Ядро свертки. Здесь C и B постоянно разные. А вот я напомню, на прошлом разборе авторы сильно использовали эту фишку, что везде C и B одинаковые, и поэтому ядро можно очень эффективно вычислить, очень эффективно свертку посчитать. Все, теперь у нас, получается, нет доступа к свертке, потому что C и B, они теперь постоянно меняются. Теперь мы должны реально по-трушному делать recurrent inference. То есть мы скатываемся в эпоху RNN, когда обучение, типа, цикл по времени постоянно был. А то это плохо. Как так? Как бы этого нельзя ни что делать. Это надо как-то компенсировать. И вот первое, что они делают, чтобы компенсировать эту сложность по времени, это они опять пишут крутой алгоритм, который учитывает эту иерархию памяти, низкого уровня код на коде и получается немножко скомпенсировать вот эту потерю что у нас теперь инференс не параллелизуемый но хотя бы утилизейшн у GPU максимальный какой возможен Я не буду рассказывать опять, как этот алгоритм работает, я потому что сам до конца этого не понимаю. Я только вот идею этого рисунка вижу, что вот это все будет храниться в быстрой памяти, а это будет храниться в медленной памяти, по кусочкам копироваться в быструю память. Это повышает утилизейшн, чтобы было меньше простоев при input-output операциях. Это первый момент. Второй момент, вот я уже говорил, теперь и inference, и обучение занимают вот от N времени, причем теперь у нас обучение как бы не параллелизуемое, свертка теперь недоступна. Как тогда быть? Не будем же мы реально цикл по времени раскручивать. Вот оказывается, можно все-таки распараллелить. Оказывается, есть вот такой алгоритм, называется параллелскан. Это алгоритм, с которым знакомы все программисты Куды и всего такого. Это алгоритм, который помогает очень эффективно распараллелить вычисление кумулятивной суммы. Кумсум – это тоже некая рекуррентная операция, как вычислить кумсум. Потому что на каждом шаге нужно знать всю предыдущую историю. И вот есть какой-то алгоритм, который в два прохода, вперед-назад, он вычисляет кумулятивную сумму на ГПУ. И получается, что там уже времени занимает не O от N, а O от N делить на степень параллелизации. И, короче, вот этот алгоритм, его можно применить для вот такой схемы, потому что, вот если взглянуть на эти формулы, они очень похожи на то, что на кумсум, на кумулятивную сумму. Здесь вот происходит очень что-то похожее на кумулятивную сумму. И вот они такие талантливые программисты, что смогли адаптировать этот алгоритм для их архитектуры. И то есть все равно inference получилось распараллелить. Вот. Может возникнуть вопрос, почему тогда обычную LSTM нельзя распараллелить с помощью этого же алгоритма. Честно, я не знаю, но мне кажется, потому что в LSTM там есть gate функции, активации, там очень сложная связь, а здесь все очень просто. Здесь просто умножение на матрицы и сложение. Поэтому, наверное, это и легко и так было распараллелить и написать этот параллелскан алгоритм. Вот. Значит, это второй ключик к эффективности MAMBA. Точнее не эффективности Mamba, а эффективности реализации вот такой схемы. Теперь мы можем говорить уже про саму Mamba. Все вместе собрать, архитектура. Значит, архитектурно Mamba выглядит вот таким образом. Она вдохновлена вот этой архитектурой H3, что у нас вот есть две SSM-ки. SSM-ка это свертка, поэтому они здесь решили вообще отказаться от SSM-ки. Здесь один D-свертка используется. Все равно функцию тоже выполняет. То есть и здесь вот у нас опять тоже две SSM-ки. И они использовали еще идею, сегодня популярную, gated MLP. Это то, что называется swish activation, swiglu activation, silu activation. Это когда мы из одного тензора получаем две проекции с помощью линейного слоя, и потом одну из них пропускаем через активацию и умножаем на вторую поэлементно. И это современный метод использовать функции активации. Вот они совместили два этих способа и получили свою архитектуру MAMBA. Здесь опять у нас есть две проекции одного и того же тензора, просто одну из этих проекций мы пропускаем через вот SSM-ку. Через две SSM. И вот этот блок SSM - это вот эти формулы, которые здесь реализованы эффективно. И, собственно, вот это и есть архитектура Mamba. Так она работает. Здесь input-output, чтобы повысить утилизацию. Здесь parallel scan во время тренинга, чтобы распараллелить все. И вот новомодные функции активации, которые используются во всех LLM. Ну, в общем-то, вот, теперь можно понять архитектуру и перейти, наконец, к экспериментам. Какие были эксперименты? Значит, всем, надеюсь, известно, что оригинальная статья Mamba, она была реджепнута с конференции вот этой вот. Почему это было? На Open Review я там смотрел и тоже мнение других экспертов ознакомился. В общем, проблема в том, что они, во-первых, сравниваются с очень старыми моделями, типа вот там GPT-Neo, какие-то PIFIA вообще, когда уже там давно есть LAMA, по крайней мере, вторая LAMA есть, может быть, и третья уже тогда была. Нет, третьей еще не было. В общем... А размер-то у них как бы, ну, похожий. Можно было с чем сравниваться. Это первое. А второе, что они сравниваются вот по перплексии. Много где. Здесь перплексия, здесь перплексия. А перплексия, она вот как бы, ну, не оценивает именно умственные способности. Она оценивает только способность моделировать язык. Типа, как хорошо следующее слово предсказывается. Вот. И поэтому статью «Реджекнули» И там есть, конечно, тоже это вроде как противоречивое мнение, вот кто-то там на Open Review тоже писал, что типа это плохая практика отвергать статью только из-за экспериментов, потому что во всем остальном статья типа отличная, вам просто эксперименты там не понравились, зачем вы так риджетнули, вот. Ну, факт в том, что потом, спустя пару месяцев или полгода, вышла полноценная статья типа сравнения Mamba с трансформерами. И вот их эксперименты я сейчас буду рассказывать. Уже как бы эту статью приняли на конференцию, с ней все в порядке. Значит, какие эксперименты были в этой статье? Они сравнивали Mamba и трансформер в трех сеттингах. На коротких контекстах, чтобы сравнить вот интеллектуальные способности. Это вот стандартные бетчмарки для LLM на длинных контекстах, чтобы сравнить, как хорошо работает то, для чего оно было создано, для длинных контекстов. Это обычно вопросно-ответный датасет, где куча-куча-куча информации, и потом взаимозависимый вопрос на основе этой информации, чтобы найти маленький кусочек и ответить. И всякие синтетические задачи, о которых чуть попозже расскажем. Итак, значит, вот... short context, чтобы проверить интеллектуальные способности. И что мы здесь видим? Мы видим, что Mamba уступает Transformer, потому что вот все эти бенчмарки, если усреднить, то видно, что Transformer лучше, причем неплохой отрыв есть. Но если приглядеться, то видно, что во всех задачах Mamba как бы выигрывает, кроме вот этих задач, MMLU, потому что эти задачи, они причем очень сильно проигрывают, тут вот на 20 пунктов практически, хотя во всех остальных местах, там, ноздря, ноздрю. И из этого они делают вывод, что Mamba хуже... подается in-context learning, потому что эти задачи, типа zero-shot, five-shot, там большой типа prompt, из которого нужно научиться получать, из которого нужно понять, как решать задачи, в каком формате выводить. И вот как бы Mamba хуже с этим справляется, поэтому здесь такой большой degradation происходит. Но оказалось, что если там обучить на датасете побольше, здесь вот был датасет, они взяли трансформер, взяли Mamba, обучили на одном и том же датасете. Датасет вот один триллион токенов. Если сделать то же самое с датасетом побольше, то уже все выравнивается. И здесь типа все неплохо, и там все неплохо. И то есть проблема с in-context-learning'ом, она немножко пропадает. И что мы можем, какой вывод сделать? Если говорить, что Mamba убийца трансформера, то, наверное, нет. Потому что вот здесь она проигрывает. Но если говорить, что Mamba типа альтернатива трансформеру, то вполне себе. Потому что она реально on-par, competitive result. Ну вот, какой можно было сделать с этого эксперимента? Какой я сделал? Дальше они проверяют мамбу на синтетических задачах с длинным контекстом. Значит, есть такая задача, фонбук, в ней на вход подается телефонная книга в формате имя-телефон, имя-телефон, и там этих телефонов несколько тысяч, ну, может, сотен. И в конце спрашивается, какой телефон у этого человека, и там имя пишется. И, в общем, вот результаты оценки моделей на этой датасейсе, и видно, что Mamba очень плохо справляется с этой задачей. Она очень быстро забывает. Причем во время притрейна там все модельки, и Mamba, и Transformer, они... обучались на длину входной последовательности 4000 токенов. И вот трансформер, и четко видно, что как только мы превышаем эту длину контекста, трансформер он плохо экстраполирует, поэтому он жестко падает сразу. А мамбы, они еще до этого падают. Короче, вообще никуда не годится. Извини, что прерву, а это трансформер с ванильными синусоидами и экосинусоидами на входе? Нет, они старались в этот трансформер взять современные практики построения LLM. Это 8B параметров. Там вот эти свиглу активации, rotary positional encoding и всякие прочие штуки, которые имитируют реальные open source трансформеры. То есть это не прям такое решение. Я предлагаю сейчас по другой ссылке перезайти, потому что у нас минута осталась. Давай. Сейчас скину канал.
