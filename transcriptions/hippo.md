Ага, всем привет! Очень рад видеть всех присутствующих на этом семинаре. Сегодня я расскажу про рекуррентную память с помощью проекций на базе сесполиномов. Статья называется HIPPO. Это одна из первых статей по теме State Space Models, которая потом приведет к MAMBA. Там всего вот между этой статьей и MAMBA около двух или трех статей. И вот в этой, как мне кажется, закладываются такие теоретические основы, база математическая всего остального. Вот. В общем, здесь будет очень много математики, конкретно функционального анализа и дифференциальных поравнений. Я решил подробно это все освежить, прямо так расписать для тех, у кого это было давно, или для тех, у кого вообще это не было. В общем, надеюсь, будет всем понятно, всем интересно. Сначала вот будет про реквизиты. Я расскажу про такие ключевые идеи из функции, которые понадобятся. Это идея разложения вектора по базису, идея разложения функции по базису, примеры базисов в функциональном пространстве, базисы там из полиномов будут, и про то, как это использовать для аппроксимации функций. И затем немножко про дифференциальное уравнение, типа идея, как выглядит само уравнение, что такое решение дифференциального уравнения, как получить это решение численно, а не аналитически. Вот на примере метода Эйлера расскажу еще. В общем-то, идея основная, которая всем, надеюсь, знакома, это разложение по байзу. Допустим, у нас есть какое-то вещественное векторное пространство, обозначается vr, и вот некоторый вектор в нем. Будем рассматривать на примере трехмерного геометрического пространства. Вектор А выделен черным. В этом пространстве можно зафиксировать некоторый базис. Н-мерный базис. Базис зен-векторов. Что это означает? Вот здесь пример стандартный базис, когда это просто параллельно осям координат, как говорится, все векторы единичной длины и перпетиплярны друг к другу. Если у нас есть байзис, то мы можем разложить любой элемент векторного пространства по этому байзису. И вот так это выглядит. B – это векторы, а C – это коэффициенты вещественные, с которыми эти векторы входят в наш вектор А. И вот все эти коэффициенты C, они сами образуют некоторый вектор n-мерный. И вот ключевое, что нужно понимать, что вот этот вектор коэффициентов, который называется вектор координат, он является крутым признаковым описанием этого исходного объекта А. Настолько крутым, что вот вообще в линейной алгебре сами ведь векторы С, они образуют свое линейное пространство. Даже не то, что линейное, а метрическое пространство. То есть у близких объектов, у близких векторов очень похожие векторы координат, очень похожие близкие признаковые описания. И это то, на чем вообще строится весь ML, что у похожих объектов похожие признаковые описания. Вот, это был пример простого геометрического пространства. В общем-то, что еще такое базис? Пару слов. Это вот базис, он определяется относительно какого-то скалярного произведения. Вот эти векторы в базисе, они ортонормированы, говорят. То есть они все друг другу перпендикулярны, и при этом длина у них единичная. Это так вот записывается. И вот эти c образуют векторное представление. Вот эти коэффициенты c, то есть коэффициенты, с которыми вектор базисный входит в вектор а, можно найти как раз-таки с помощью скалярного произведения. Это легко доказывается. Если вот сюда вместо а подставить эту линейную комбинацию, то все слагаемые сократятся, кроме одного, которым будет вектор c. И вот эти коэффициенты c на картинке изображены как ax, ay, az. То есть это те самые проекции на уси координат. В общем, надеюсь, это всем понятно было, все легко. И, в общем, оказывается, что те же самые действия разложения на базис, нахождение этих коэффициентов c, можно произвести еще и с функциями. Функции — это, как известно, бесконечно верное пространство. И обычно, когда говорят о функциях, говорят о пространствах Либега. Вот L2 — это дважды интегрированная по Либегу функция на отрезке от минус одного до одного. В этом пространстве тоже можно ввести всякие операции, сложения, то есть линейные операции. И более того, можно ввести скалярные произведения на этом пространстве. И оно будет выглядеть вот таким образом, что это интеграл по всей области определения произведения значений функций с учетом какой-то весовой функции wx. Интуиция за этим интегралом такая, что обычное скалярное произведение в конечномерном пространстве это сумма произведений координат, а в бесконечномерном пространстве, в непрерывном причем пространстве, это вот тоже сумма, только непрерывная сумма интегралов. И вместо произведения координат, произведение самих значений функций. Вот такая простая идея, как видите, скалярное произведение. В общем, как только мы ввели скалярное произведение, мы сразу можем говорить о бадисе. Бадисе в пространстве этих функций. Значит, базис... Так-так-так. В общем, вот примеры самых разных базисов, которые можно ввести в пространстве функций. Вот всем известный тригонометрический базис из синусов и косинусов. С помощью формулы Эллера это можно записать так, где и это мимо единицы. В общем, вот эти функции, которые бесконечное число, они образуют базис на отрезке от 0 до 2π. Что это означает? Что любую функцию определенную на таком отрезке можно разложить в бесконечную линейную комбинацию из таких вот функций. Вот пример этой формулы, бесконечная линейная комбинация. Есть еще много разных базисов, и именно на них мы и сосредоточимся, потому что это базисы из полиномов. Вот самый известный – это полиномы Лежандра. Они образуют базис на отрезке от минус 1 до 1, и вот так вот они определяются. Это явная формула. Здесь вот это производная, то есть n-ная производная от вот такой n-ной степени, x квадрат минус 1. Формула тут у них у всех очень страшная, очень сложная. Но суть в том, что если вы возьмете два таких полинома, посчитаете их скалярное произведение относительно их весовой функции, то вы получите либо 0, либо 1. 1 только если скалярное произведение самим собой считается. И еще что нужно отметить, что они на разных множествах определены. Где-то это какой-то ограниченный отрезок от одного до одного, а где-то это целая полупрямая или вообще вся вещественная прямая. И, соответственно, функции весовые у них разные, вид разный и так далее. Кто не знал или кто забыл. Применяется это, как я сказал, что можно разложить любую функцию по базису. Вот, допустим, у нас есть функция из пространства Либека на отрезке. Пусть это какой-то артно-нормированный базис. И тогда мы можем разложить функцию f на такую линейную комбинацию. И вот эти буквы c, их можно все вытянуть в один бесконечный вектор. И этот вектор тоже будет вектор координат признакового описания. Очень хорошее признаковое описание. Получить эти векторы буквы c, коэффициенты c, можно тоже с помощью скалярного произведения. То есть мы скалярно умножаем наш объект на какой-то базисный элемент, базисный полином. Вот этот интеграл. И получаем то, насколько много данного полинома в данной функции. Что нам сейчас важно? То, что у нас вектор бесконечный, а мы хотим решать какие-то прикладные задачи, поэтому нам нужен конечный вектор. То есть нам нужна какая-то аппроксимация к конечномерным пространствам. И оказывается, что в случае с этими всеми байдесами это сделать очень легко. Чтобы получить n-мерную аппроксимацию, достаточно просто оборвать вот этот вот ряд на первых n слагаемых. И все. И вот эти c, это будет вектор координат конечный, который будет приближать. Причем это будет оптимальная аппроксимация с точки зрения расстояния между функциями. То есть между вот этой функцией и между исходной функцией расстояние минимально среди всех возможных вот таких вот комбинаций полиномов. И, ну, как вы понимаете, с помощью этих c можно, значит, еще и как-то приближенно восстановить исходную функцию f, просто поставив в это выражение. Вот, в общем-то, это все было из функона, что нужно знать. Дальше про дифференциальное уравнение. Значит, дифференциальное уравнение – это вот уравнение такого вида. Вот в обычных алгебраических уравнениях у нас неизвестные это числа, вектора и все такое. А в дифференциальном уравнении неизвестная это целая функция. Вот x от t. Мы хотим найти целую функцию. И причем задано это уравнение в таком виде, что у нас есть производная x, и вот эта производная равняется какой-то другой функции. То есть мы знаем, с какой скоростью изменяется функция x. И надана по условию какая-то функция f в правой части. И когда мы решим это репертуарное уравнение, называется, когда мы его проинтегрируем, мы найдем какой-то общий вид или какой-то конкретный вид функции x. И эта функция x обычно описывает какую-то динамику, динамику физической системы или какой-то еще другой. И если исходно функция f, то это правая часть уравнения, была какой-то адекватной гладкой функцией, то и решение будет тоже какое-то гладкое, предсказуемое. То есть вот я пример привел, это так называемая фазовая траектория, то есть, грубо говоря, это визуализация функции x во времени, как она меняется. Мы видим, что сама функция сложная очень, но что не отнять, это то, что она непрерывная. Она непрерывная, такая гладкая и предсказуемая. То есть если мы отойдем небольшой шаг во времени из этой точки, то мы окажемся недалеко от этой точки. Вот. Это про аналитическое решение. Но мы же хотим опять решать прикладные задачи, поэтому нам нужно как-то численно уметь решать. И для этого тоже существует схема. Как численно найти решение гиперсоциального уравнения. Вот вместе с вами сейчас мы это даже выведем. Это очень легко. В общем, просто берем и расписываем по определению производным в левой части. То есть вот это производная, правая часть. И мы только левую часть изменили. Это по определению производной это предел разностного отношения. вот здесь вот разность приращение функции это приращение аргумента dt предполагается какой-то конечно малый и просто мы тогда умножаем все уравнение на dt x от переносим правую сторону и получаем вот такое выражение оно означает что если мы заглянем в будущее на момент dt то значение нашей функции выражается следующим образом. Это предыдущее значение плюс производное умножить на некоторую константу dt от скаляр. Тут есть некоторый референс с градиентным спуском, что это производное, то есть градиент некоторый, а это learning rate. И вот так выражается локальная связь. И дальше мы можем что сделать? Мы можем весь таймлайн, всю t, разбить на равномерную сетку, шаг равный dt, то есть шаг дискретизации, и мы можем численно найти значение функции x в каждый момент времени t. Вот так это выражается. И опять это очень похоже на градиентный спуск. Все такое. И что это означает? Что мы как бы просимулировали систему какую-то, некоторые движения. Мы как бы рекуррентно, это рекуррентная формула, узнали, как будет двигаться какой-то объект. Как вот система некоторая будет изменяться. Ну и вот напоследок еще пример. В общем, слева это уравнение на уровне школьной физики. Второй закон Ньютона и все. Тут ничего сложного нет. То есть вот Производная x это скорость по x, производная y это скорость по y. А производная скорости это ускорение. А ускорение это f делить на m, силу делить на массу. Тут вот представлена динамика для квадрокоптера. Квадрокоптер, двумерный квадрокоптер, у него всего две. Симуляция такая двумерная на плоскости. Координаты x и y. У1 и У2 это сила, с которой два пропеллера тянут его вверх. Ну или вниз. И в общем, вот мы можем составить эти уравнения. И затем по методу Эйлера их решить, эти дифференциальные уравнения. То есть опять новое состояние х, это старые координаты, плюс маленький шажок по скорости. Скорость умножить на время пройденное. И так мы можем просимулировать движение какого-то квадрокоптера. Собственно, это было все про математические пререквизиты. Надеюсь, все было понятно, познавательно, интересно. Теперь можно говорить про саму статью. Значит, HIPPO расшифровывается High Order Polynomial Projections, полином, все такое. И можно вот так вот на три шага разделить всю статью концептуальных. Значит, во-первых, это постановка задачи. Авторы рассматривают задачу работы с временными рядами, то есть классификация временных рядов и предсказание временных рядов, какое-то такое авторегрессионное. Вот, это во-первых. Во-вторых, они говорят, за каждым временным рядом стоит некоторая непрерывная функция. Поэтому мы будем анализировать непрерывную функцию. Из этого пойдем. И потом вот ключевой шаг, что давайте использовать в качестве памяти аппроксимацию всей предыдущей истории функции от нуля до какого-то момента времени. И вот коэффициент этой аппроксимации, это и будет вектор нашей памяти. Вот. Собственно, начинаем. У нас есть какая-то непрерывная функция f от t. Мы хотим, вот как разные модели работают с этой функцией. Если мы говорим про трансформер, то трансформер смотрит на всю эту историю, на всю функцию f, и у него, по сути, нет никакого механизма памяти. Все на всех смотрят, и дальше идет классификация или предсказание. А вот рекуррентные модели, они шаг за шагом просматривают каждый кусок этой функции и по каким-то рекуррентным формулам обрабатывают. И вот в рекуррентных моделях как раз-таки появляется механизм памяти. И какой у нее цель? Какой смысл вообще памяти? И вот авторы говорят, память нужна, чтобы помнить всю предыдущую историю функции. То есть вот знать, как функция себя вела в предыдущие моменты времени. Поэтому идея, что если мы просто возьмем и всю эту историю, всю эту функцию разложим в ряд исполиномов. И коэффициенты, с которыми эти полиномы входят, мы будем использовать как вектор состояния, как признаковое описание, как механизм памяти. Потому что эти коэффициенты оптимальным образом запоминают все, что было до этого. Это ли не круто? Вот это основная идея. Давайте в качестве памяти использовать просто коэффициенты разложения по базису из полиномов. Окей, зафиксировали. Что тут нужно сказать? Какова идея? Давайте в каждый момент времени производить разложение. Тогда мы получим вектор c, это будет некоторая функция. Вектор c это векторная функция от момента времени. В каждый момент времени будем получать это c. Но проблема, мы же хотим какие-то рекуррентные формулы иметь, чтобы из текущего c выразить предыдущее c. Из предыдущего c выразить текущее. То есть, какой-то эффективный формул для пересчета. И вот оказывается, что вот этот вектор C, если провести теоретические выкладки, оказывается, что он описывается дифференциальным уравнением, некоторой динамикой. C с точкой – это производная по времени. И вот этот вектор C, он описывается вот такой системой, вот таким дифференциальным уравнением. Здесь A и B, они определяются базисом, которым мы пользуемся. Лежандра, парилома Лагера и что-нибудь еще подобное. А вклад функции f, он здесь явный. То есть вот тут явное выражение, как f влияет на коэффициенты c. Ну, с учетом... В общем, вот. И что тут важно сказать? Мы говорили, что диаперсальные уравнения, они выдают какое-то решение, какую-то функцию. Вот функция c от t. И эта функция, она как бы гладкая. Если вот исходная функция f достаточно гладкая, то решение c, оно такое гладкое, предсказуемое и все такое. Если исходная функция f это тоже какой-то там белый шум, то и вот эта c тоже будет изменяться очень хаотически. Вот, это важно понимать. Ну и матризы A и B, они, то есть вот, они определяют, они, еще раз, не зависят от функции F, они определяются самой задачей. То есть мы можем один раз какие-то аналитические выкладки провести, найти эти матризы A и B, и для любой функции F уметь получать C, обрабатывать любую последовательность входную. Вот, окей, это про неправильный случай. Опять-таки мы должны перейти в дискретный случай. И как это сделать? Просто берем и применяем техники для численного решения дифферок. То есть мы берем весь наш таймлайн, разбиваем его равномерно, шаг дискретизации dt, и просто применяем, например, простейший метод Эйлера. И получаем опять классную рекуррентную формулу, где новое состояние выражается как предыдущее состояние плюс шаг вдоль градиента. А и Б опять нужно просто вычислить. А и Б четко связаны с этими А и Б, которые здесь были. И вот это general framework, так скажем, как нам нужно действовать. Берем, фиксируем базис полиномов, находим теоретически матрицы А и В, подставляем их и, грубо говоря, обработка последовательности сводится к численному решению дифференциального уравнения. Вот. Такой важный момент, чиппоинт, надеюсь, понятно было. Дальше уже поговорим про конкретные, как вот найти эти матрицы А и Б, какие они получаются и так далее. Вот, надеюсь, можем дальше идти. В общем, перед этим надо пару технических деталей обговорить. Первое, это про то, что базис полиномов на каком-то конкретном отрезке определен от минус 1 до 1 или от 0 до 2π. А нам надо, чтобы этот базис постоянно менялся, потому что функция от 0 до tau определена. Мы хотим постоянно функцию разную, то есть на разном пространстве мы хотим раскладывать. И второе, мы хотим какой-то реализовать минимальный механизм внимания, чтобы все-таки не всю историю запоминать, а более свежей информацией уделять больше внимания. Вот. Собственно, Вот, напоминаю, полиномы, они определены на каком-то отрезке всегда. Или на прямой, либо на полупрямой. Мы хотим, чтобы это было определено только на нашем отрезке, на котором нам надо. И тут идея очень простая. Просто берем и применяем какое-то линейное преобразование аргумента. И тогда наша функция перемещается из минус 1 до 1 до желаемого нами отрезка. Вот, надеюсь, это понятно и просто. Дальше идея... А, ну вот визуализация этой идеи, что у нас было так, теперь как нам надо. Дальше идея, надо уделять внимание, все-таки не нужно все запоминать, нужно только последнюю историю как-то учитывать. И оказывается, с помощью этого механизма с полиномами можно реализовать простейшие механизмы скользящего окна и взвешивания. В общем-то, с помощью того же преобразования аргумента мы можем отобразить наш базис из исходного отрезка просто в последний какой-то контекст. И вот так использовать. Меняется момент времени, и за нами же окно искользит. Базис всегда можно привести к нужному нам отрезку. И дальше проблема, что можно еще использовать вот эти weight функции, wbx, которые до этого были обозначены. В общем, для полиномов Лежандра вы могли вспомнить, заметить, что здесь weight функции – это константа. Здесь вот один, здесь один. А вот для полиномов Лагера уже, например, экспоненциальная функция. Для Чебышева там другая функция. В общем, что это означает? Это означает, что в нашем скалярном произведении, в этом интеграле, мы всем моментом времени уделяем одинаковое внимание, если у нас weight функция константная. А если weight функция экспоненциальная, то тогда более свежей истории мы уделяем больше внимания. Потому что с большим коэффициентом такое слагаемое уходит в наш интеграл. А в более поздней истории мы уделяем меньше внимания. И так вот вместе с нашим скользящим окном это все изменяется. Ну и у них еще третья идея была, что давайте все-таки от нуля до tau использовать весь контекст. И тогда у нас будет weight функция так визуализирована. Почему здесь высота постоянно меняется? Потому что эта weight функция должна интегрироваться в единицу, как плотность распределения. Поэтому здесь у всех этих прямоугольников интеграл 1. Интеграл равен единице. Ну, в общем-то, вот. Это, кстати, как раз и три версии модели, которые авторы тестировали. Лежандр, лагер и шкалированный лежандр. Дальше я уже буду говорить про... Ну да, вот как они реализованы, вот эти три версии. Значит, первая версия. Значит, зафиксировали. Значит, у нас пространство – это скользящее окно размера τ, размера θ. θ – это размер окна. Мы вот туда все полиномы перемещаем. в качестве базиса используем полином или жанра, тогда вот эта весовая функция в общем случае это просто единица. Но поскольку у нас окно теперь другого размера, у нас весовая функция такая. Это вот скаляр умножить на индикатор. Индикатор того, что мы находимся в этом окне. Что у нас момент времени, он не больше, чем этот и не меньше, чем этот. И вот если эти три пункта зафиксировать, то очень сложными теоретическими выкладками... можно получить явные выражения для матриц А, которые причем не зависят от времени. То есть каждое время у нас будет вот такая матрица А. И тут вот как бы формула для всех координат, то есть для всех компонентов матрицы А. N-ная строка, катый столбец, вот так определяется, такой формулой. И B, это у нас вектор, вот такой формулой определяется. Там, в общем, это теоретический вывод, он дан в аппендиксе. Он непростой в том смысле, что там... нужно знать все вот эти свойства полиномов лежандра то что полиномы лежандра они через друг друга выражаются там тоже рекуррентная форма у них есть что их производные тоже выражаются что там интегралы чему-то равны в общем жонглирование самыми разнообразными фактами из лунка на из мод анализа оно вот позволяет получить вот эти выражения для матрицы а и для вектора b если выбрать другой базис Вот, если использовать полиномы Лагера на таком отрезке, то получается вот такая weight функция, вот получается такая матрица, вообще из нулей единиц, ну, с учетом этого коэффициента. Хотя его здесь нет, это опечатка, прошу прощения. Здесь уже tau, это с прошлого стало, все просто копипастили. Здесь просто 1, 0 должно быть. То есть ниже диагонали в этой матрице единицы, а выше диагонали нули. А вектор b вообще все из единиц, вот. И последний их вариант, это если использовать вот такой базис. Кажется, время у нас скоро закончится, да? Ничего. Если использовать вот такой базис, то получается другая матрица. Опять нужно производить по-новому выкладки все и получать явные выражения. Собственно, вот все теория. Дальше я хочу рассказать про, собственно, эксперименты, которые они проводили. Какие результаты добились кто-нибудь. Надеюсь, еще кто-нибудь слушает. Так, эксперименты. Сначала расскажу про то, как вообще использовать эту модель. В общем, HIPPO – это у нас просто память, регулярная память. Ее надо встроить как-то. И они предлагают, в общем, как ее использовать. Вот таким образом. Значит, у нас есть обычная RNN-ячейка. У нее есть hidden state, у нее есть вход какой-то и один выход. Она работает вот по таким формулам. Значит, G - это наш gate, это сегмойда, то есть число от 0 до 1, оно регулирует, сколько информации мы запоминаем и забываем. Надеюсь, всем это знакомо. И вот, значит, мы, то есть, предыдущий hidden state взвешиваем на этот gate, а вот это вот новая информация, которая нам пришла с обходом x. Это гиперболические танги, соответственно, от 1 до 1. Вот, то есть простейшая архитектура RNN. И вот в нее они интегрируют HIPPO. Причем как интегрируют? Они просто вместо входа X еще добавляют этот вектор контекста C. То есть вот здесь был просто HX, а здесь H и к X приконкатинируют еще вектор C. То есть C это как раз-таки то, что это HIPPO, то, что по нашей динамике работает. И вектор C, он работает на какой-то одномерной функции. Мы пока что только одномерные функции смотрели. А у нас, типа, в RNN-ке нигде нет одномерной функции. У нас все там многомерно. Поэтому они такой, коспили немного ход, сделали. Они вот в качестве одномерной функции рассматривают скалярное произведение hidden state с обучаемым вектором. То есть я не беру скалярное произведение всех вот этих hidden state-ов, умножают их на один и тот же вектор w, который обучается. И вот у нас одномерная функция есть. И типа вот у этой функции, считаем, что она очень репрезентативна, что очень важно ее запоминать, у этой функции считается вот эта рекуррентная память. И она подается на вход. В виде схемы это вот так можно визуализировать, что хиппо, это функция f, она получается из наших hidden states, на вход хиппо получает f и c, то есть вот c и f. И потом этот C с X он конкатенируется и подается на вход R-энергии. Вот такая схема. Такая слабая схема, конечно. В следующих статьях они более подробно раскроют потенциал лучше. Пока что довольствуемся этим. И вот с такой архитектурой они везде экспериментируют. В общем-то, теперь эксперименты. Быстренько. Ну, скорее всего, время у вас выйдет. Зуму придется переподключаться. В общем, первый эксперимент — это permuted MNIST. В общем, у нас всем известный MNIST, датасет, рукописными цифрами. В чем датасет состоит? Мы берем картинку цифры, вытягиваем это в один вектор и применяем какую-то перестановку, рандомный шафл. И получается, грубо говоря, вот такой шум вместо картинки. И вот одну и ту же перестановку мы применяем ко всем объектам датасета. Зачем это нужно? Это нужно для того, чтобы воспринимать картинку как временной ряд, одномерный временной ряд. А поскольку пересновка везде одинаковая, то в этом ряде все-таки закодирована исходная информация о числах, какие были нарисованы. И они, значит, рассматривали задачу классификации такого временного ряда. То есть по вот этой вытянутой шумной последовательности, ну, не шумной, перемьюченной последовательности, нужно предсказать, какая цифра была там, то есть 10 классов. Ну и, собственно, задача временной ряд. Вот они соту побили. Вот, значит, здесь это вот их интеграция с RNN-кой. Это другие RNN-подходы. А здесь вот трансформер. Это свертка TCN Temporal Convolutional Network. А это не знаю, что такое. В общем, они всех бьют. Видно, что трансформер, типа, получше, чем все RNN-ки. Но их RNN-ка лучше даже, чем трансформер. Вот. Ну, вот такая сота. Задача, конечно, немножко игрушечная. Потом задача на память, задача копинга. У нас есть последовательность L + 20 цифр каких-то нарисованных. Первые 10 элементов этой последовательности - это какие-то 10 цифр, которые нужно запомнить. Потом, грубо говоря, все, что после них - это шум какой-то идет. И в конце мы должны в авторегрессионном формате выдать, какие первые 10 цифр нам встретились там. Вот, чисто задача на память: сможет ли он вспомнить, что было в начале, несмотря на весь шум, который был. И они там просто в тексте написали, что, типа, наша модель almost perfectly работает на этой задаче, но никаких метрик не предоставили. Вот, статья на NIPS. Потом другая задача: классификация траекторий. У нас дана опять временной ряд, уже трехмерный временной ряд. В общем, один ряд - это одна буква какая-то, рукописная. И записано что? Записано какой-то частотой, сэмплируются точки вот на этой траектории. То есть координаты в плоскости и степень нажатия ручки в этот момент. И вот имея этот временный ряд, опять нужно предсказать, какая буква была нарисована. В общем-то, опять они там всех рвут и мечут. Причем они эту задачку интересно очень поставили, что они обучали модель на одной частоте ассамплирования, а тестируется на другой. То есть вот обучали на 100, а тестируется на 200 Гц и так далее. И видно, что все другие модели, они как бы вообще не выдерживают такой out of distribution, а у них спокойно все решается. Вот. И даже пропуски в значениях не сильно, даже почему-то лучше. Ну ладно, наверное, здесь данные в разных строчках разные. В общем, даже пропуски в значениях хорошо обрабатывает эта моделька. Потом, ладно, это сложный слайд, я долго его буду объяснять. Вот, наконец-то, слайд про языковое моделирование. Вот есть IMDB-датасет, это датасет с отзывами на кинофильмы. Нужно провести классификацию. Какой это отзыв? Положительный, нейтральный, негативный? И что они пишут? Что их модель, она на уровне с LSTM-ками, не хуже. Но феноменальное качество, соток, пробить не удалось. Это единственный эксперимент во всей статье, связанный с языковым моделированием. Все остальное – это просто какие-то временные ряды. И последнее – это опять временной ряд, уже предсказание на 15 шагов вперед, надо предсказать. временной ряд связанный с какой-то динамической системой. Типа какая-то хаотическая динамическая система. Ну и вот опять у них самая маленькая ошибка среди всех. Очень крутые. LSTM это просто LSTM регулярная архитектура. LMU это тоже какая-то такая навороченная RNN архитектура, специально созданная для работы с временными рядами. А это типа гибридный вариант, когда там слои LSTM и слои LMU чередуются постоянно. И вот даже вот этот гибрид страшного монстра, они еще свои модели смогли победить. Лекес — это Лежандра с Кейвуд. И, в принципе, все. Это все результаты. Я могу, если у нас еще время есть, я вот не знаю, как посмотреть, сколько у нас времени осталось в конференции. Кто-то может сказать? Две минуты. Две минуты. Успею. Короче, заключение, как мне кажется. Хотя лучше давайте сейчас сразу переподключимся. Сейчас надо разобраться мне, как это сделать. Ты сможешь пересоздать? Давай, наверное, я ссылку пересоздам. Да, давай. Все, тогда переключаемся, ссылка сейчас будет в канале.
