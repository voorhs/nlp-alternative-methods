@misc{gu2020hipporecurrentmemoryoptimal,
      title={HiPPO: Recurrent Memory with Optimal Polynomial Projections}, 
      author={Albert Gu and Tri Dao and Stefano Ermon and Atri Rudra and Christopher Re},
      year={2020},
      eprint={2008.07669},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2008.07669}, 
}

@misc{gu2021combiningrecurrentconvolutionalcontinuoustime,
      title={Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers}, 
      author={Albert Gu and Isys Johnson and Karan Goel and Khaled Saab and Tri Dao and Atri Rudra and Christopher Ré},
      year={2021},
      eprint={2110.13985},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.13985}, 
}

@misc{gu2022efficientlymodelinglongsequences,
      title={Efficiently Modeling Long Sequences with Structured State Spaces}, 
      author={Albert Gu and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2111.00396},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.00396}, 
}

@misc{gupta2022diagonalstatespaceseffective,
      title={Diagonal State Spaces are as Effective as Structured State Spaces}, 
      author={Ankit Gupta and Albert Gu and Jonathan Berant},
      year={2022},
      eprint={2203.14343},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.14343}, 
}

@misc{gu2022parameterizationinitializationdiagonalstate,
      title={On the Parameterization and Initialization of Diagonal State Space Models}, 
      author={Albert Gu and Ankit Gupta and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2206.11893},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.11893}, 
}

@misc{gu2024mambalineartimesequencemodeling,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2024},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752}, 
}

@misc{dao2024transformersssmsgeneralizedmodels,
      title={Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality}, 
      author={Tri Dao and Albert Gu},
      year={2024},
      eprint={2405.21060},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.21060}, 
}

@inproceedings{
  anonymous2025mamba,
  title={Mamba-3: Improved Sequence Modeling using State Space Principles},
  author={Anonymous},
  booktitle={Submitted to The Fourteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=HwCvaJOiCj},
  note={under review}
}

@misc{smith2023simplifiedstatespacelayers,
      title={Simplified State Space Layers for Sequence Modeling}, 
      author={Jimmy T. H. Smith and Andrew Warrington and Scott W. Linderman},
      year={2023},
      eprint={2208.04933},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.04933}, 
}

@misc{elfwing2017sigmoidweightedlinearunitsneural,
      title={Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning}, 
      author={Stefan Elfwing and Eiji Uchibe and Kenji Doya},
      year={2017},
      eprint={1702.03118},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1702.03118}, 
}

@misc{shazeer2020gluvariantsimprovetransformer,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.05202}, 
}

@misc{fu2023hungryhungryhipposlanguage,
      title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models}, 
      author={Daniel Y. Fu and Tri Dao and Khaled K. Saab and Armin W. Thomas and Atri Rudra and Christopher Ré},
      year={2023},
      eprint={2212.14052},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.14052}, 
}

@misc{katharopoulos2020transformersrnnsfastautoregressive,
      title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, 
      author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
      year={2020},
      eprint={2006.16236},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.16236}, 
}

@misc{yang2025parallelizinglineartransformersdelta,
      title={Parallelizing Linear Transformers with the Delta Rule over Sequence Length}, 
      author={Songlin Yang and Bailin Wang and Yu Zhang and Yikang Shen and Yoon Kim},
      year={2025},
      eprint={2406.06484},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.06484}, 
}

@misc{kimiteam2025kimilinearexpressiveefficient,
      title={Kimi Linear: An Expressive, Efficient Attention Architecture}, 
      author={Kimi Team and Yu Zhang and Zongyu Lin and Xingcheng Yao and Jiaxi Hu and Fanqing Meng and Chengyin Liu and Xin Men and Songlin Yang and Zhiyuan Li and Wentao Li and Enzhe Lu and Weizhou Liu and Yanru Chen and Weixin Xu and Longhui Yu and Yejie Wang and Yu Fan and Longguang Zhong and Enming Yuan and Dehao Zhang and Yizhi Zhang and T. Y. Liu and Haiming Wang and Shengjun Fang and Weiran He and Shaowei Liu and Yiwei Li and Jianlin Su and Jiezhong Qiu and Bo Pang and Junjie Yan and Zhejun Jiang and Weixiao Huang and Bohong Yin and Jiacheng You and Chu Wei and Zhengtao Wang and Chao Hong and Yutian Chen and Guanduo Chen and Yucheng Wang and Huabin Zheng and Feng Wang and Yibo Liu and Mengnan Dong and Zheng Zhang and Siyuan Pan and Wenhao Wu and Yuhao Wu and Longyu Guan and Jiawen Tao and Guohong Fu and Xinran Xu and Yuzhi Wang and Guokun Lai and Yuxin Wu and Xinyu Zhou and Zhilin Yang and Yulun Du},
      year={2025},
      eprint={2510.26692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2510.26692}, 
}

@misc{qwen32next,
      title={Qwen-3.2-next},
      author={qwen team},
      year={2025},
      url={https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list},
}

@misc{qiu2025gatedattentionlargelanguage,
      title={Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free}, 
      author={Zihan Qiu and Zekun Wang and Bo Zheng and Zeyu Huang and Kaiyue Wen and Songlin Yang and Rui Men and Le Yu and Fei Huang and Suozhi Huang and Dayiheng Liu and Jingren Zhou and Junyang Lin},
      year={2025},
      eprint={2505.06708},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.06708}, 
}

@misc{yang2025gateddeltanetworksimproving,
      title={Gated Delta Networks: Improving Mamba2 with Delta Rule}, 
      author={Songlin Yang and Jan Kautz and Ali Hatamizadeh},
      year={2025},
      eprint={2412.06464},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.06464}, 
}

@misc{rwkv,
      title={RWKV Architecture History},
      author={RWKV Wiki},
      url={https://wiki.rwkv.com/basic/architecture.html},
}

@misc{peng2023rwkvreinventingrnnstransformer,
      title={RWKV: Reinventing RNNs for the Transformer Era}, 
      author={Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Jiaju Lin and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Bolun Wang and Johan S. Wind and Stanislaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
      year={2023},
      eprint={2305.13048},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13048}, 
}