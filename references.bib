@misc{gu2020hipporecurrentmemoryoptimal,
      title={HiPPO: Recurrent Memory with Optimal Polynomial Projections}, 
      author={Albert Gu and Tri Dao and Stefano Ermon and Atri Rudra and Christopher Re},
      year={2020},
      eprint={2008.07669},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2008.07669}, 
}

@misc{gu2021combiningrecurrentconvolutionalcontinuoustime,
      title={Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers}, 
      author={Albert Gu and Isys Johnson and Karan Goel and Khaled Saab and Tri Dao and Atri Rudra and Christopher Ré},
      year={2021},
      eprint={2110.13985},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.13985}, 
}

@misc{gu2022efficientlymodelinglongsequences,
      title={Efficiently Modeling Long Sequences with Structured State Spaces}, 
      author={Albert Gu and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2111.00396},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.00396}, 
}

@misc{gupta2022diagonalstatespaceseffective,
      title={Diagonal State Spaces are as Effective as Structured State Spaces}, 
      author={Ankit Gupta and Albert Gu and Jonathan Berant},
      year={2022},
      eprint={2203.14343},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.14343}, 
}

@misc{gu2022parameterizationinitializationdiagonalstate,
      title={On the Parameterization and Initialization of Diagonal State Space Models}, 
      author={Albert Gu and Ankit Gupta and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2206.11893},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.11893}, 
}

@misc{gu2024mambalineartimesequencemodeling,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2024},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752}, 
}

@misc{dao2024transformersssmsgeneralizedmodels,
      title={Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality}, 
      author={Tri Dao and Albert Gu},
      year={2024},
      eprint={2405.21060},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.21060}, 
}

@inproceedings{
  anonymous2025mamba,
  title={Mamba-3: Improved Sequence Modeling using State Space Principles},
  author={Anonymous},
  booktitle={Submitted to The Fourteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=HwCvaJOiCj},
  note={under review}
}

@misc{smith2023simplifiedstatespacelayers,
      title={Simplified State Space Layers for Sequence Modeling}, 
      author={Jimmy T. H. Smith and Andrew Warrington and Scott W. Linderman},
      year={2023},
      eprint={2208.04933},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.04933}, 
}

@misc{elfwing2017sigmoidweightedlinearunitsneural,
      title={Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning}, 
      author={Stefan Elfwing and Eiji Uchibe and Kenji Doya},
      year={2017},
      eprint={1702.03118},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1702.03118}, 
}

@misc{shazeer2020gluvariantsimprovetransformer,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.05202}, 
}

@misc{fu2023hungryhungryhipposlanguage,
      title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models}, 
      author={Daniel Y. Fu and Tri Dao and Khaled K. Saab and Armin W. Thomas and Atri Rudra and Christopher Ré},
      year={2023},
      eprint={2212.14052},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.14052}, 
}

@misc{katharopoulos2020transformersrnnsfastautoregressive,
      title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, 
      author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
      year={2020},
      eprint={2006.16236},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.16236}, 
}