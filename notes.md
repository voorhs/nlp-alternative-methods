# State-Space Models: От HiPPO к Mamba

## Аннотация

Данная лекция представляет всесторонний обзор State-Space Models (SSMs) в глубоком обучении, прослеживая их эволюцию от основополагающего фреймворка HiPPO до современной архитектуры Mamba. Мы начинаем с математических основ полиномиальных проекций и оптимальных механизмов памяти, затем исследуем, как эти концепции были адаптированы в эффективные архитектуры для моделирования последовательностей. Лекция охватывает три основных развития: рекуррентную память HiPPO с оптимальными полиномиальными проекциями, переход к структурированным слоям пространства состояний (LSSL, S4, DSS, S4D), и наконец, селективные модели пространства состояний, воплощенные в Mamba. На протяжении всего изложения мы рассматриваем как теоретические основы, так и практические реализации, подчеркивая, как эти модели достигают линейной временной сложности, сохраняя при этом конкурентоспособную производительность с Transformers на различных задачах моделирования последовательностей.

## Содержание

1. [Математические предпосылки](#математические-предпосылки)
2. [HiPPO: Рекуррентная память с оптимальными полиномиальными проекциями](#hippo-рекуррентная-память-с-оптимальными-полиномиальными-проекциями)
3. [State-Space Models: От теории к практике](#state-space-models-от-теории-к-практике)
4. [Mamba: Селективные пространства состояний](#mamba-селективные-пространства-состояний)
5. [Заключение](#заключение)

---

## Математические предпосылки

### Основы функционального анализа

Прежде чем погрузиться в State-Space Models, необходимо установить математические основы, которые делают эти архитектуры возможными. Ключевые концепции включают:

**Разложение по базису в векторных пространствах**

Рассмотрим векторное пространство $V(\mathbb{R})$ с базисом $B = \{\vec{b}_1, \ldots, \vec{b}_N\}$. Любой вектор $\vec{a} \in V$ может быть выражен как:

$$\vec{a} = c_1\vec{b}_1 + \ldots + c_N\vec{b}_N$$

где $\vec{c} = (c_1, \ldots, c_N)$ — это вектор координат, представляющий $\vec{a}$ в базисе $B$.

Если базис ортонормированный:
$$\langle \vec{b}_i, \vec{b}_j \rangle = \begin{cases} 1, & i = j \\ 0, & i \neq j \end{cases}$$

то коэффициенты могут быть вычислены как:
$$c_n = \langle \vec{a}, \vec{b}_n \rangle$$

**Функциональные пространства и ортонормированные базисы**

Те же принципы распространяются на функциональные пространства. Для функций $f, g \in L^2[-1,1]$ мы определяем скалярное произведение как:

$$\langle f, g \rangle = \int_{-1}^1 f(x)g(x) w(x) dx$$

где $w(x)$ — весовая функция. Общие ортонормированные полиномиальные базисы включают:

- **Полиномы Лежандра**: $L^2[-1,1]$ с $w(x) = 1$
- **Полиномы Лагера**: $L^2[0,+\infty)$ с $w(x) = e^{-x}$
- **Полиномы Чебышева**: $L^2[-1,1]$ с $w(x) = \frac{1}{\sqrt{1-x^2}}$
- **Полиномы Эрмита**: $L^2(-\infty,+\infty)$ с $w(x) = e^{-x^2}$

**Аппроксимация функций**

Для любой функции $f(x) \in L^2[-1,1]$ и ортонормированного базиса $\{P_n(x)\}_{n=1}^\infty$ оптимальная аппроксимация:

$$f(x) = \sum_{n=1}^\infty c_n P_n(x) \approx \sum_{n=1}^N c_n P_n(x)$$

где коэффициенты образуют признаковое представление:
$$c_n = \langle f(x), P_n(x) \rangle = \int_{-1}^1 f(x)P_n(x)w(x)dx$$

### Дифференциальные уравнения и численное интегрирование

**Обыкновенные дифференциальные уравнения**

ОДУ описывают динамику системы:
$$\frac{d}{dt}x(t) = f(x(t), t)$$

Решение ОДУ (численно или аналитически) называется интегрированием.

**Метод Эйлера**

Простейшая схема численного интегрирования:
$$\frac{x(t+dt) - x(t)}{dt} = f(x(t), t)$$

Это дает нам рекуррентное соотношение:
$$x_{k+1} = x_k + f(x_k, k)dt$$

где $dt$ — шаг дискретизации.

**Другие методы интегрирования**

- **Обратный Эйлер**: $\bar{A} = (I - Adt)^{-1}$, $\bar{B} = (I - Adt)^{-1}Bdt$
- **Билинейный метод**: $\bar{A} = \left(I - A\frac{dt}{2}\right)^{-1}\left(I + A\frac{dt}{2}\right)$
- **Zero-order hold (ZOH)**: $\bar{A} = e^{Adt}$, $\bar{B} = (\bar{A} - I)A^{-1}B$

---

## HiPPO: Рекуррентная память с оптимальными полиномиальными проекциями

### Основные концепции

HiPPO (High Order Polynomial Projections) вводит революционный подход к рекуррентной памяти, используя коэффициенты разложения как состояния памяти. Ключевая идея заключается в том, чтобы рассматривать одномерные временные ряды как непрерывные функции и использовать полиномиальные проекции для оптимального сжатия исторической информации.

**Фреймворк HiPPO**

1. **Представление одномерной функции**: Каждый временной ряд рассматривается как непрерывная функция $f(t)$
2. **Коэффициенты разложения как память**: Вместо хранения сырой истории мы храним коэффициенты $c(t)$ полиномиального разложения
3. **Рекуррентная память как динамика ОДУ**: Коэффициенты эволюционируют согласно дифференциальным уравнениям
4. **Численное решение**: Используем численное интегрирование для обновления состояний памяти

### Математическая формулировка

Для функции $f(t)$, определенной на $[0, \tau]$, мы стремимся аппроксимировать её, используя полиномиальные базисные функции. Оптимальные коэффициенты $c(t)$ удовлетворяют:

$$\frac{d}{dt}c(t) = Ac(t) + Bf(t)$$

где $A$ и $B$ — матрицы, определяемые выбранным полиномиальным базисом и методом интегрирования.

**Дискретизация**

Применяя численное интегрирование (например, метод Эйлера):
$$c_{n+1} = c_n + (Ac_n + Bf_n)dt = (I + Adt)c_n + (Bdt)f_n$$

Полагая $\bar{A} = I + Adt$ и $\bar{B} = Bdt$, получаем:
$$c_{n+1} = \bar{A}c_{n-1} + \bar{B}f_n$$

### Реализации HiPPO

**Translated Legendre (LegT)**
- Пространство: $L^2[\tau-\theta, \tau]$ (скользящее окно)
- Весовая функция: $w(t) = \frac{1}{\theta}[\tau-\theta \leq t \leq \tau]$
- Элементы матрицы:
  $$A_{nk} = \frac{1}{\theta}\begin{cases}
  (-1)^{n-k}(2n+1), & n \geq k \\
  2n+1, & n \leq k
  \end{cases}$$
  $$B_n = \frac{1}{\theta}(2n+1)(-1)^n$$

**Translated Laguerre (LagT)**
- Пространство: $L^2[-\infty, \tau]$ (экспоненциальное взвешивание)
- Весовая функция: $w(t) = \exp(t-\tau)[t \leq \tau]$
- Элементы матрицы:
  $$A_{nk} = \begin{cases}
  1, & n \geq k \\
  0, & n < k
  \end{cases}$$
  $$B_n = 1$$

**Scaled Legendre (LegS)**
- Пространство: $L^2[0, \tau]$ (полная история)
- Весовая функция: $w(t) = \frac{1}{\tau}[0 \leq t \leq \tau]$
- Элементы матрицы:
  $$A_{nk} = -\frac{1}{\tau}\begin{cases}
  \sqrt{(2n+1)(2k+1)}, & n > k \\
  n+1, & n = k \\
  0, & n < k
  \end{cases}$$
  $$B_n = \sqrt{2n+1}$$

### Интеграция с нейронными сетями

HiPPO был интегрирован с RNN путем использования коэффициентов разложения как дополнительного контекста:

$$\text{RNN}(h, [c, x])$$

где $c_t = \bar{A}c_{t-1} + \bar{B}f_t$ представляет коэффициенты HiPPO для $f(t) = w^T h_t$ (с обучаемым $w$).

### Экспериментальные результаты

HiPPO достиг результатов уровня SOTA на нескольких бенчмарках:

- **Permuted MNIST**: SOTA производительность на классификации последовательностей изображений
- **Классификация траекторий**: Робастная производительность при различных частотах дискретизации
- **Предсказание Mackey Glass**: Превосходные возможности долгосрочного предсказания
- **IMDB Sentiment**: Конкурентоспособная производительность на классификации текста

Ключевые преимущества:
- Быстрый рекуррентный инференс через интегрирование ОДУ
- Оптимальное сжатие памяти через полиномиальные проекции
- Теоретические гарантии качества аппроксимации

---

## State-Space Models: От теории к практике

### Линейные слои пространства состояний (LSSL)

Основываясь на HiPPO, LSSL трансформирует теоретический фреймворк в практический слой нейронной сети.

**Отображение последовательностей**

SSM отображает входную последовательность $u$ в выходную последовательность $y$:
$$\begin{align}
x_t &= \bar{A} x_{t-1} + \bar{B} u_t \\
y_t &= Cx_t + Du_t
\end{align}$$

где:
- $\bar{A} \in \mathbb{R}^{N \times N}$, $\bar{B} \in \mathbb{R}^N$ (опционально обучаемые)
- $C \in \mathbb{R}^{M \times N}$, $D \in \mathbb{R}^M$ (обучаемые)

**Рекуррентность как свертка**

Разворачивание рекуррентности показывает, что SSM реализуют свертку:
$$y_t = C(\bar{A})^t\bar{B}u_0 + C(\bar{A})^{t-1}\bar{B}u_1 + \ldots + C\bar{B}u_t$$

Это может быть записано как:
$$y = \mathcal{K}_L(\bar{A}, \bar{B}, C) * u + Du$$

где ядро:
$$\mathcal{K}_L(A, B, C) = (CB, CAB, \ldots, CA^{L-1}B) \in \mathbb{R}^{M \times L}$$

**Вычислительная сложность**

- **Обучение**: $O(L \log L)$ через FFT свертку
- **Инференс**: $O(L)$ через рекуррентность

Эта двойственная природа позволяет эффективное параллельное обучение при сохранении быстрого последовательного инференса.

### Стратегии параметризации

**Трехдиагональная параметризация (LSSL)**

Ключевая идея заключается в том, что матрицы HiPPO могут быть представлены как:
$$A = P(D + T^{-1})Q$$

где $D$, $P$, $Q$ — диагональные, а $T$ — трехдиагональная. Это сокращает параметры с $N^2$ до $6N$, сохраняя теоретические свойства.

**Normal Plus Low-Rank (S4)**

S4 использует параметризацию:
$$A = V\Lambda V^* - PQ^*$$

где $\Lambda$ — диагональная, $V$ — унитарная, а $P, Q \in \mathbb{R}^{N \times r}$ — матрицы низкого ранга. Это позволяет эффективное вычисление ядра через специализированные алгоритмы.

**Диагональная параметризация (DSS, S4D)**

Самый простой подход использует диагональные матрицы:
$$A = \text{diag}(\lambda_1, \ldots, \lambda_N)$$

Это позволяет чрезвычайно эффективное вычисление ядра:
$$K_k = \sum_{i=0}^{N-1} C_i \bar{A}_i^k \bar{B}_i$$

### Стратегии инициализации

Все методы инициализируются от матриц HiPPO:

**Инициализация S4D**
- **S4D-Inv**: $A_{nn} = -\frac{1}{2} + i\frac{N}{\pi}\left(\frac{N}{2n+1} - 1\right)$
- **S4D-Lin**: $A_{nn} = -\frac{1}{2} + i\pi n$

### Экспериментальные результаты

**Long-Range Arena**: S4 достиг прорывных результатов на задачах классификации очень длинных последовательностей, значительно превзойдя предыдущие методы.

**Языковое моделирование**: Первоначальные эксперименты показали конкурентоспособную перплексию с Transformers, будучи при этом в 60 раз быстрее при генерации.

**Обработка аудио**: Прямая обработка сырых аудиоволн (дискретизация 16кГц) без спектральной предобработки.

---

## Mamba: Селективные пространства состояний

### Мотивация и постановка проблемы

Хотя SSM показали перспективность, они страдали от фундаментального ограничения: **инвариантности по времени**. Параметры $A$, $B$, $C$ оставались постоянными независимо от входа, что затрудняло селективное запоминание или забывание информации на основе контекста.

**Задача селективного копирования**

Рассмотрим задачу, где модель должна селективно копировать токены на основе контекста. Традиционные SSM испытывают трудности, поскольку не могут адаптировать свой механизм памяти к содержимому входа.

### Предыстория: Linear Attention

До Mamba, Linear Attention показал, как эффективно аппроксимировать внимание Transformer:

**Стандартное внимание**
$$\text{Attention}(Q, K, V) = \text{softmax}(QK^T)V$$

**Аппроксимация Linear Attention**
Заменим $\text{sim}(q, k) = \exp(q^T k)$ на $\text{sim}(q, k) = \phi(q)^T \phi(k)$:

$$O_i = \frac{\phi(Q_i)^T \sum_{j=1}^i \phi(K_j)V_j^T}{\phi(Q_i)^T \sum_{j=1}^i \phi(K_j)}$$

Это позволяет рекуррентное вычисление:
$$\begin{align}
S_i &= S_{i-1} + \phi(K_i)V_i^T \\
Z_i &= Z_{i-1} + \phi(K_i)
\end{align}$$

### Hungry Hungry Hippos (H3)

H3 связал SSM и языковое моделирование, аппроксимируя внимание компонентами SSM:

**Слой H3**
$$Q \circ \text{SSM}_{\text{diag}}(\text{SSM}_{\text{shift}}(K) \circ V)$$

Эта архитектура:
- Аппроксимирует внимание Transformer, используя SSM
- Достигает конкурентоспособных результатов на SuperGLUE
- Использует Flash Convolution для эффективного использования GPU

### Архитектура Mamba

**Селективные пространства состояний**

Mamba вводит входозависимые параметры:

$$\begin{align}
x_t &= \bar{A}(x_t) x_{t-1} + \bar{B}(x_t) u_t \\
y_t &= C(x_t) x_t + D(x_t) u_t
\end{align}$$

где $A$, $B$, $C$ теперь являются функциями от входа $x_t$.

**Детали реализации**

- **Функции параметров**: Простые линейные проекции от входа
- **Селективный механизм**: Ворота контролируют поток информации
- **Эффективная реализация**: Пользовательские CUDA ядра для оптимизации иерархии памяти

**Алгоритм параллельного сканирования**

Поскольку свертка больше недоступна, Mamba использует параллельное сканирование для распараллеливания рекуррентности:
- Позволяет $O(N)$ параллельное вычисление во время обучения
- Сохраняет линейно-временной инференс
- Адаптирует алгоритмы кумулятивной суммы к рекуррентности SSM

### Компоненты архитектуры Mamba

**Полная архитектура**
1. **Проекция входа**: Линейный слой к скрытой размерности
2. **Селективный SSM**: Входозависимое вычисление пространства состояний
3. **Gated MLP**: Современные функции активации (SiLU/Swish)
4. **Проекция выхода**: Обратно к исходной размерности

**Оптимизация иерархии памяти**

Mamba реализует сложное управление памятью:
- **SRAM**: Быстрая память на чипе для активных вычислений
- **HBM**: Высокопропускная память для хранения параметров
- **Объединенные операции**: Минимизируют передачи памяти

### Экспериментальная оценка

**Задачи короткого контекста**

На стандартных бенчмарках языкового моделирования (библиотека Harness):
- **Конкурентоспособная производительность**: Наравне с Transformers на большинстве задач
- **Обучение в контексте**: Немного слабее Transformers
- **Масштабирование**: Разрыв в производительности закрывается с большими датасетами

**Задачи длинного контекста**

- **Вопросно-ответные задачи**: Конкурентоспособность на задачах QA с длинным контекстом
- **Синтетические задачи**: Сильная производительность на бенчмарках Phonebook и RULER
- **Экстраполяция**: Лучшая экстраполяция длины, чем у Transformers

**Гибридные архитектуры**

Комбинирование Mamba с компонентами Transformer:
- **Лучшее из двух миров**: Объединяет эффективность Mamba с возможностями Transformer
- **Улучшенная производительность**: Лучшие результаты, чем чистая Mamba
- **Гибкий дизайн**: Позволяет смешивать различные архитектурные компоненты

### Характеристики производительности

**Преимущества**
- **Быстрый инференс**: Линейно-временная сложность
- **Эффективность памяти**: Постоянное использование памяти
- **Экстраполяция длины**: Лучше, чем Transformers
- **Низкая перплексия**: Конкурентоспособное языковое моделирование

**Ограничения**
- **Обучение в контексте**: Слабее Transformers
- **Размытая память**: Менее точная, чем механизмы внимания
- **Чувствительность к промптам**: Более чувствительна к форматированию входа

---

## Заключение

Эволюция от HiPPO к Mamba представляет замечательное путешествие в моделировании последовательностей, демонстрируя, как теоретические инсайты могут быть трансформированы в практические архитектуры, которые бросают вызов доминированию Transformers.

### Ключевые вклады

**HiPPO**: Установил теоретическую основу, используя оптимальные полиномиальные проекции для рекуррентной памяти, предоставив принципиальный подход к сжатию исторической информации.

**State-Space Models**: Связали теорию и практику, показав, как рекуррентная динамика может быть реализована как эффективные свертки, позволяя параллельное обучение при сохранении последовательного инференса.

**Mamba**: Ввел селективные механизмы, которые позволяют моделям адаптировать свою память на основе содержимого входа, достигая конкурентоспособной производительности с линейно-временной сложностью.

### Технические инсайты

1. **Математические основы**: Полиномиальные проекции обеспечивают оптимальное сжатие исторической информации
2. **Двойственная природа**: Двойственность рекуррентность-свертка позволяет эффективное обучение и инференс
3. **Селективные механизмы**: Входозависимые параметры критичны для обработки сложных паттернов последовательностей
4. **Иерархия памяти**: Тщательное внимание к организации памяти GPU существенно для практической эффективности

### Будущие направления

Успех Mamba и связанных архитектур предполагает несколько перспективных направлений:

- **Гибридные архитектуры**: Объединение сильных сторон различных архитектурных парадигм
- **Специализированные приложения**: Адаптация архитектур для конкретных доменов (например, генерация кода)
- **Законы масштабирования**: Понимание того, как эти модели масштабируются с параметрами и данными
- **Теоретический анализ**: Более глубокое понимание того, почему селективные механизмы работают

### Практические последствия

Для практиков State-Space Models предлагают:

- **Эффективность**: Линейно-временная сложность для длинных последовательностей
- **Гибкость**: Могут быть объединены с другими архитектурными компонентами
- **Масштабируемость**: Лучшее использование памяти, чем Transformers для длинных контекстов
- **Конкурентоспособная производительность**: Достигают результатов, сравнимых с Transformers на многих задачах

Поле продолжает быстро эволюционировать, с новыми архитектурами, строящимися на этих основах. По мере продвижения вперед принципы, установленные HiPPO, усовершенствованные через семейство State-Space Models и доведенные до совершенства в Mamba, вероятно, будут влиять на следующее поколение архитектур для моделирования последовательностей.

---

*Эта лекция проследила теоретические основы и практические реализации, которые привели от полиномиальных проекций к современным селективным моделям пространства состояний, демонстрируя, как математические инсайты могут стимулировать архитектурные инновации в глубоком обучении.*
